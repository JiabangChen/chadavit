{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Channel Adaptive Vision Transformer: How to Use </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a step-by-step guide on how to use the Channel Adaptive Vision Transformer (ChAdaViT) model for image classification. The ChAdaViT model is a vision transformer that can adaptively take as input images from different number of channels, and project them into the same embedding space. This is particularly useful when working with multi-channel images, such as medical microscopy or even geopspatial images with multiple modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:40:48.253919900Z",
     "start_time": "2025-08-06T10:40:48.234097700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "from src.backbones.vit.chada_vit import ChAdaViT\n",
    "# 会依次去执行src，backbones，vit中的__init__()文件，但是后两者的init文件在src的init文件中已经会执行过一次，因此就不会再执行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:40:52.276583800Z",
     "start_time": "2025-08-06T10:40:52.209431600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download weights\n",
    "You can download the model weights under this URL: https://drive.google.com/file/d/1SUfUwerHJlf0vo9mdgM0mRn9TNZkaqXl/view?usp=drive_link   \n",
    "Make sure to download it on the same directory as this notebook, and give the right permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the path of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:40:55.265742700Z",
     "start_time": "2025-08-06T10:40:55.227167800Z"
    }
   },
   "outputs": [],
   "source": [
    "CKPT_PATH = \"weights.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the hash of the downloaded file here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:40:57.855176100Z",
     "start_time": "2025-08-06T10:40:57.828696400Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_hash(file_path, expected_hash):\n",
    "    md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        while chunk := f.read(4096):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:41:03.488368200Z",
     "start_time": "2025-08-06T10:41:00.648294600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_hash(CKPT_PATH, \"e8a24ac58b8e34bdce10e0024d507f2e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:41:06.082248200Z",
     "start_time": "2025-08-06T10:41:06.045537400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "PATCH_SIZE = 16\n",
    "EMBED_DIM = 192\n",
    "RETURN_ALL_TOKENS = False\n",
    "MAX_NUMBER_CHANNELS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:41:10.479783400Z",
     "start_time": "2025-08-06T10:41:09.563938900Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ChAdaViT(\n",
    "    patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    return_all_tokens=RETURN_ALL_TOKENS,\n",
    "    max_number_channels=MAX_NUMBER_CHANNELS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:41:40.052617700Z",
     "start_time": "2025-08-06T10:41:39.054603300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChAdaViT(\n",
      "  (token_learner): TokenLearner(\n",
      "    (proj): Conv2d(1, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=192, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=192, bias=True)\n",
      "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.0, inplace=False)\n",
      "      (dropout2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "assert (\n",
    "    CKPT_PATH.endswith(\".ckpt\")\n",
    "    or CKPT_PATH.endswith(\".pth\")\n",
    "    or CKPT_PATH.endswith(\".pt\")\n",
    ") # 确保checkpoint文件后缀名正确\n",
    "state = torch.load(CKPT_PATH, map_location=\"cpu\")[\"state_dict\"]\n",
    "# 从checkpoint文件中找出模型的预训练参数，然后保存到state中\n",
    "for k in list(state.keys()):\n",
    "    if \"encoder\" in k:\n",
    "        state[k.replace(\"encoder\", \"backbone\")] = state[k]\n",
    "    if \"backbone\" in k:\n",
    "        state[k.replace(\"backbone.\", \"\")] = state[k]\n",
    "    del state[k]\n",
    "# 这里是把state中的键的名字从encoder.xxx或者backbone.xxx改成xxx，相当于把前缀去掉，使得其键的名字转换为能对应当前 model 中参数的名字。\n",
    "model.load_state_dict(state, strict=False) # state中还有DINO训练时留下来的teacher model的参数，\n",
    "# 即以momentum_开头的参数，它们是由训练时student的参数通过EMA训练得到的，但是我现在只需要用student model\n",
    "# 的参数就好了，因此strict=False，这样可以根据model中的实际所需参数名来加载，无需加载teacher类的参数了。\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(model)\n",
    "# TODO: 是不是只有Moyen大小的模型，没有petite和grand大小的模型，如果我想调整token维数或者attn头数是不是不行？因为没有符合这样结构的预训练权重集？\n",
    "# TODO: 预训练好的模型参数是不是只能适配十个channel的，如果我只想做两个channel的是不是不行 ---是的 不行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Images (Optional)\n",
    "If you are here, you probably want to test the model with your own images :)      \n",
    "But anyway, you can use the following code to generate random images with different number of channels to simply check if the model is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:25:24.154732300Z",
     "start_time": "2025-08-06T13:25:24.013065500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(num_images: int, max_num_channels=MAX_NUMBER_CHANNELS):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for i in range(num_images): # 对每一张图做个遍历\n",
    "        num_channels = np.random.randint(1, max_num_channels + 1) # 随机给某张图赋上channel值\n",
    "        imgs.append(torch.randn(num_channels, 224, 224)) # 随机给某张图赋上每一个channel的H和W上的pixel值\n",
    "        labels.append(torch.randint(0, 1, (1,))) # 随机给此图赋上label，这个label在[0,1)中随机取（取整数），且这个label的形状是（1，）\n",
    "    data = list(zip(imgs, labels)) # zip就是把每一个随机生成的imgs（C,H,W）和它对应的类（1，）打包生成一个元组，然后十个元组生成一个list为data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:25:36.609832600Z",
     "start_time": "2025-08-06T13:25:36.437678400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated images: 10 \n",
      " Distribution of number of channels: {9: 1, 2: 1, 10: 2, 4: 1, 1: 1, 7: 1, 6: 2, 3: 1}\n"
     ]
    }
   ],
   "source": [
    "data = generate_data(num_images=10, max_num_channels=MAX_NUMBER_CHANNELS) # 假设生成一个图像集，里面有10张channel不一致的图像，channel数的最大值是10\n",
    "imgs, labels = zip(*data) \n",
    "# 这里的意思是把data这个列表中的元素（是一个个元组）解包之后再传给zip即变成zip((img1, label1), (img2, label2), (img3, label3))，这个会把所有第 0 个元素打包成一组（即 imgs），第 1 个元素打包成一组（即 labels）：imgs = (img1, img2, img3)，labels = (label1, label2, label3)\n",
    "distribution = {}\n",
    "for img in imgs:\n",
    "    num_channels = img.shape[0]\n",
    "    distribution[num_channels] = distribution.get(num_channels, 0) + 1\n",
    "print(\n",
    "    f\"Number of generated images: {len(imgs)} \\n Distribution of number of channels: {distribution}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key elements of the ChAdaViT model is the ability to adapt to different number of channels. In this section, we will prepare the data to be fed into the model. We will use the `torchvision` library to load the data, and then we will create a custom dataset that will adapt the images to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:26:01.984547500Z",
     "start_time": "2025-08-06T13:26:01.942960100Z"
    }
   },
   "outputs": [],
   "source": [
    "# 把按照一个batch输入的image整理拆分成一个channel list，label list，以及一个映射表，即每个图分别有几个channel\n",
    "def collate_images(batch: list):\n",
    "    \"\"\"\n",
    "    Collate a batch of images into a list of channels, a list of labels and a mapping of the number of channels per image.\n",
    "    \n",
    "    Args:\n",
    "        batch (list): A list of tuples of (img, label)\n",
    "\n",
    "    Return:\n",
    "        channels_list (torch.Tensor): A tensor of shape (X*num_channels, 1, height, width)\n",
    "        labels_list (torch.Tensor): A tensor of shape (batch_size, )\n",
    "        num_channels_list (list): A list of the number of channels per image\n",
    "    \"\"\"\n",
    "    num_channels_list = [] # 一个list，告诉model每个图有几个channel，形状为(batch_size, )\n",
    "    channels_list = []\n",
    "    labels_list = [] # 一个list，告诉model每个图的label，形状为(batch_size, )\n",
    "\n",
    "    # Iterate over the list of images and extract the channels\n",
    "    for image, label in batch: \n",
    "        # batch是一个列表，其中每一个元素是一个元组（imgs（形状为（C,H,W)的tensor），labels（形状为（1，）的tensor））\n",
    "        labels_list.append(label) # 提取每一个图的label，输入到label_list中\n",
    "        num_channels = image.shape[0] #提取每一个图的channel大小，输入到num_channels_list中\n",
    "        num_channels_list.append(num_channels) \n",
    "\n",
    "        for channel in range(num_channels):\n",
    "            channel_image = image[channel, :, :].unsqueeze(0) \n",
    "            # 提取某张图的某一个channel，形状为（H,W），然后再前面加一个维度，变成（1，H,W),并加入到channels_list中\n",
    "            channels_list.append(channel_image)\n",
    "\n",
    "    channels_list = torch.cat(channels_list, dim=0).unsqueeze(\n",
    "        1\n",
    "    )  # Shape: (X*num_channels, 1, height, width)\n",
    "    # 先按照第0维级联，变成（X*num_channels,height, width），X*num_channels为每张图中有几个channel，然后加和，即这整个batch输入中有多少个channel，然后在第一维上加一个维度，这个维度（值为1）应该是channel数量，即对于batch中的每一个sample而言，输入给模型的是一组一个channel的图的集合，模型初始的patch embedding也是对一个channel的图做的，只不过要对多个一channel的图做多次，而不是像传统vit一样，输入是一个多channel的图，对一个多channel的图做一次patch embedding。但前面的X*num_channels则表示为整个batch中有多少个channel，可以把如今的(X*num_channels, 1, height, width)类比为原始的（B,C,H,W），只不过以前是batch中一个sample对应B中的1个元素，如今是对应X*num_channels中的多个元素（即多个一channel图）\n",
    "    batched_labels = torch.tensor(labels_list) # 把labels_list转成torch格式\n",
    "\n",
    "    return channels_list, batched_labels, num_channels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:26:05.863454Z",
     "start_time": "2025-08-06T13:26:05.743551400Z"
    }
   },
   "outputs": [],
   "source": [
    "collated_batch = collate_images(data) # 注意，data的数据格式是一个列表，里面的每个元素是一个元组，（img，label），img和label都是张量一个是（C,H,W），一个是（1，）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:26:08.645824200Z",
     "start_time": "2025-08-06T13:26:08.557655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[9, 2, 10, 4, 1, 7, 6, 3, 10, 6]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated_batch[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:26:12.227436700Z",
     "start_time": "2025-08-06T13:26:12.201378900Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(\n",
    "    model: nn.Module,\n",
    "    batch: torch.Tensor,\n",
    "    mixed_channels: bool,\n",
    "    return_all_tokens: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    Forwards a batch of images X and extracts the features from the backbone.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to forward the images through.\n",
    "        X (torch.Tensor): The input tensor of shape (batch_size, 1, height, width).\n",
    "        list_num_channels (list): A list of the number of channels per image.\n",
    "        index (int): The index of the image to extract the features from.\n",
    "        mixed_channels (bool): Whether the images have mixed number of channels or not.\n",
    "        return_all_tokens (bool): Whether to return all tokens or not.\n",
    "\n",
    "    Returns:\n",
    "        feats (Dict): A dictionary containing the extracted features.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Overwrite model \"mixed_channels\" parameter for evaluation on \"normal\" datasets with uniform channels size\n",
    "    model.mixed_channels = mixed_channels # 这里的model是来自chada_vit.py的model，初始化时并没有mixed_channels这个属性，这里是从外部加了这个属性\n",
    "\n",
    "    X, targets, list_num_channels = batch # 对应于上文的channels_list, batched_labels, num_channels_list\n",
    "    # X = X.to(device, non_blocking=True) # jiabang‘s change,我的dataloader中不会设置pin_memory=True,这里也不要non_blocking=True\n",
    "    X = X.to(device)\n",
    "    targets = targets.to(device) # jiabang's change, 我是要做supervised fine-tune的，因此要把targets也放到gpu上\n",
    "    feats = model(x=X, index=0, list_num_channels=[list_num_channels])\n",
    "    # index应该是这个batch的起始索引，即从第几个开始是此batch的内容，一般为0\n",
    "    if not mixed_channels:\n",
    "        if return_all_tokens:\n",
    "            # Concatenate feature embeddings per image\n",
    "            chunks = feats.view(sum(list_num_channels), -1, feats.shape[-1])\n",
    "            chunks = torch.split(chunks, list_num_channels, dim=0)\n",
    "            # Concatenate the chunks along the batch dimension\n",
    "            feats = torch.stack(chunks, dim=0)\n",
    "        # Assuming tensor is of shape (batch_size, num_tokens, backbone_output_dim)\n",
    "        feats = feats.flatten(start_dim=1) # 这里是保留batch这个维度，然后把后面维度拉成一个长的向量，比如若feats是（B,C,H,W），那么就会变成（B,CxHxW），这里是（B，1xbackbone_output_dim）\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:27:03.404816800Z",
     "start_time": "2025-08-06T13:26:19.408524500Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_features = extract_features(\n",
    "    model=model,\n",
    "    batch=collated_batch,\n",
    "    mixed_channels=True, # 是否一个batch中的图的channel会存在不一样的情况，我的情况一般为false jiabang's alert\n",
    "    return_all_tokens=RETURN_ALL_TOKENS, # 一般都不需要把所有token都return出来 我只要cls_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:27:12.751677800Z",
     "start_time": "2025-08-06T13:27:12.722584400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 embeddings of dim 192 were extracted.\n"
     ]
    }
   ],
   "source": [
    "assert extracted_features.shape[0] == len(\n",
    "    collated_batch[2]\n",
    ")  # num_embeddings == num_images, even with different number of channels，就是batch size不会变\n",
    "print(\n",
    "    f\"{extracted_features.shape[0]} embeddings of dim {extracted_features.shape[1]} were extracted.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chada-vit-dmL_hUKa-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
