{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:18:27.611042500Z",
     "start_time": "2025-08-09T10:18:27.515223600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_grad_cam\n",
    "import torch.hub as hub\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.backbones.vit.chada_vit import ChAdaViT\n",
    "import hashlib\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "channels = 3\n",
    "batch_size = 32\n",
    "baseline = False # whether to use baseline model: VGG-16, DenseNet-121,ResNet-50, efficientnet_v2_small, convnext_base\n",
    "baseline_model = ['densenet121', 'efficientnet_v2_s', 'resnet50', 'vgg16', 'convnext_base']\n",
    "early_stop_mode = 'loss or accuracy'  # choose 'loss' mode, 'accuracy' mode, 'loss or accuracy' mode or 'loss and accuracy' mode, here using weighted F1-score to replace accuracy, especially in imbalanced dataset\n",
    "# Number of variations to generate per image\n",
    "num_variations_per_image_0 = 1\n",
    "num_variations_per_image_1 = 8\n",
    "test_percent = 0.15  # choose the proportion size of test set\n",
    "validation_percent = 0.1  # choose the proportion size of validation set if close the cross validation\n",
    "cross_validation = True  # choose open or close Cross-Validation\n",
    "fold_num = 5  # choose the number of fold if open the cross-validation\n",
    "if not cross_validation:\n",
    "    fold_num = 1\n",
    "CKPT_PATH = \"weights.ckpt\"\n",
    "mixed_channels = False\n",
    "\n",
    "dataset_path = \"D:\\cell_images_manual_extracted\"\n",
    "whole_dataset_path = \"D:\\cell_image_augmented_manual_extracted\"\n",
    "train_images_path = \"D:\\cell_image_train_manual_extracted\\\\\"\n",
    "test_images_path = \"D:\\cell_image_test_manual_extracted\\\\\"\n",
    "grad_cam_base_path = \"D:\\cell_image_XAI\\Grad-CAM\\\\\"\n",
    "augmented_cancer_path = 'D:\\cell_image_augmented\\cancer\\\\'\n",
    "augmented_normal_path = 'D:\\cell_image_augmented\\\\normal\\\\'\n",
    "\n",
    "dataset_autoseg_path = \"D:\\\\cell_autoseg\"\n",
    "train_autoseg_path = \"D:\\\\cell_autoseg_train\\split\"\n",
    "train_autoseg_cancer_path = 'D:\\\\cell_autoseg_train\\split\\\\cancer\\\\'\n",
    "train_autoseg_normal_path = 'D:\\\\cell_autoseg_train\\split\\\\normal\\\\'\n",
    "train_autoseg_cv_path = 'D:\\\\cell_autoseg_train\\\\cross validation\\\\'\n",
    "test_autoseg_path = 'D:\\\\cell_autoseg_test\\split'\n",
    "test_whole_autoseg_path = 'D:\\\\cell_autoseg_test\\whole\\\\'\n",
    "test_autoseg_cancer_path = \"D:\\\\cell_autoseg_test\\split\\\\cancer\\\\\"\n",
    "test_autoseg_normal_path = 'D:\\\\cell_autoseg_test\\split\\\\normal\\\\'\n",
    "validation_autoseg_path = 'D:\\\\cell_autoseg_validation\\split'\n",
    "validation_whole_autoseg_path = 'D:\\\\cell_autoseg_validation\\whole\\\\'\n",
    "validation_autoseg_cancer_path = 'D:\\\\cell_autoseg_validation\\split\\\\cancer\\\\'\n",
    "validation_autoseg_normal_path = 'D:\\\\cell_autoseg_validation\\split\\\\normal\\\\'\n",
    "validation_autoseg_cv_path = 'D:\\\\cell_autoseg_validation\\\\cross validation\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_hash(file_path, expected_hash):\n",
    "    md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        while chunk := f.read(4096):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest() == expected_hash"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:18:31.064385Z",
     "start_time": "2025-08-09T10:18:31.041253900Z"
    }
   },
   "id": "c66b4e160865c6cc",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_hash(CKPT_PATH, \"e8a24ac58b8e34bdce10e0024d507f2e\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:18:35.782748900Z",
     "start_time": "2025-08-09T10:18:32.384147300Z"
    }
   },
   "id": "d90a488f3f5a2454",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def collate_images(batch: list):\n",
    "    \"\"\"\n",
    "    Collate a batch of images into a list of channels and a mapping of the number of channels per image.\n",
    "    \n",
    "    Args:\n",
    "        batch (list): A batch of images Tensor(B,C,H,W)\n",
    "\n",
    "    Return:\n",
    "        channels_list (torch.Tensor): A tensor of shape (X*num_channels, 1, height, width)\n",
    "        num_channels_list (list): A list of the number of channels per image\n",
    "    \"\"\"\n",
    "    num_channels_list = []\n",
    "    channels_list = []\n",
    "\n",
    "    # Iterate over the list of images and extract the channels\n",
    "    for image in batch: \n",
    "        num_channels = image.shape[0]\n",
    "        num_channels_list.append(num_channels) \n",
    "\n",
    "        for channel in range(num_channels):\n",
    "            channel_image = image[channel, :, :].unsqueeze(0) \n",
    "            channels_list.append(channel_image)\n",
    "\n",
    "    channels_list = torch.cat(channels_list, dim=0).unsqueeze(\n",
    "        1\n",
    "    )  # Shape: (X*num_channels, 1, height, width)\n",
    "\n",
    "    return channels_list, num_channels_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T13:59:38.068958400Z",
     "start_time": "2025-08-09T13:59:38.010792900Z"
    }
   },
   "id": "a7e1a2f7842bed3b",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_mean_std(ds, channels):  # calculate the mean and std for each channel in the dataset\n",
    "    mean = torch.zeros(channels)\n",
    "    std = torch.zeros(channels)\n",
    "    for image in ds:\n",
    "        for channel in range(channels):\n",
    "            mean[channel] += image[channel, :, :].mean()\n",
    "            std[channel] += image[channel, :, :].std()\n",
    "    mean = mean / len(ds)\n",
    "    std = std / len(ds)  #TODO: is this a correct way to calculate STD?\n",
    "    return mean, std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:19:02.270184300Z",
     "start_time": "2025-08-09T10:19:02.198452800Z"
    }
   },
   "id": "6e1a4b00a8efae74",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ResizeWithPadding:\n",
    "    def __init__(self, size, fill=0):\n",
    "        \"\"\"\n",
    "        size: tuple (width, height) target size\n",
    "        fill: pixel value to fill\n",
    "        \"\"\"\n",
    "        self.target_width, self.target_height = size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # obtain the size of original image\n",
    "        orig_width, orig_height = img.size\n",
    "\n",
    "        # use the smaller ratio to scale the width and height equally.\n",
    "        width_ratio = self.target_width / orig_width\n",
    "        height_ratio = self.target_height / orig_height\n",
    "        if width_ratio <= height_ratio:\n",
    "            new_width = int(orig_width * width_ratio + 0.1)  # plus 0.1 to prevent the float error\n",
    "            new_height = int(orig_height * width_ratio)\n",
    "        else:\n",
    "            new_width = int(orig_width * height_ratio)\n",
    "            new_height = int(orig_height * height_ratio + 0.1)\n",
    "\n",
    "        # resize\n",
    "        img = F.resize(img, [new_height, new_width])  # the resize in F needs the format of input as (height, width)\n",
    "\n",
    "        # calculate padding size\n",
    "        pad_left = (self.target_width - new_width) // 2\n",
    "        pad_top = (self.target_height - new_height) // 2\n",
    "        pad_right = self.target_width - new_width - pad_left\n",
    "        pad_bottom = self.target_height - new_height - pad_top\n",
    "\n",
    "        # 添加 padding\n",
    "        img = F.pad(img, [pad_left, pad_top, pad_right, pad_bottom], fill=self.fill)\n",
    "        \n",
    "        assert img.size[0] == self.target_width and img.size[1] == self.target_height, 'Output Image size is incorrect!'\n",
    "        \n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:19:03.419312800Z",
     "start_time": "2025-08-09T10:19:03.382929200Z"
    }
   },
   "id": "96f869130cdeab77",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the directory containing images\n",
    "transform = v2.Compose([ResizeWithPadding((224, 224)), v2.ToTensor()])\n",
    "# resize and transfer to tensor\n",
    "dataset = torchvision.datasets.ImageFolder(dataset_autoseg_path, transform=transform)  # read data\n",
    "# Assuming images are organized in subdirectories where each subdirectory name is the class label\n",
    "# 0 is cancer, 1 is normal\n",
    "\n",
    "transform_augmented = v2.Compose([v2.RandomHorizontalFlip(),\n",
    "                                  v2.RandomVerticalFlip(),\n",
    "                                  v2.RandomRotation(degrees=40),\n",
    "                                  v2.RandomAffine(degrees=40, translate=(0.1, 0.1), shear=(-8, 8, -8, 8),\n",
    "                                                  scale=(0.9, 1.1)),\n",
    "                                  #v2.RandomPerspective(distortion_scale=0.1),\n",
    "                                  #v2.GaussianBlur(kernel_size=2)\n",
    "                                  #v2.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2,hue=0)\n",
    "                                  # 0.2 means the brightness / contrast / saturation alternation range from [0.8~1.2] of original.\n",
    "                                  ])  # Image Augmented Transformation\n",
    "# here is a shuffle\n",
    "# TODO: try other augmentation method"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:19:04.410394Z",
     "start_time": "2025-08-09T10:19:04.295399600Z"
    }
   },
   "id": "cbbb4c1a98d7ad7a",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 892 cancer cells and 167 normal cells, 1059 cells in total, for training and validating\n",
      "we have 158 cancer cells and 30 normal cells, 188 cells in total, for testing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "for i in dataset.samples:\n",
    "    image_paths.append(i[0])\n",
    "    image_labels.append(i[1])\n",
    "\n",
    "# split dataset into test set and (train set + validation set)\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
    "    image_paths, image_labels, test_size=test_percent, stratify=image_labels, random_state=seed)\n",
    "# here is a shuffle\n",
    "\n",
    "print(\n",
    "    f'we have {train_val_labels.count(0)} cancer cells and {train_val_labels.count(1)} normal cells, {len(train_val_labels)} cells in total, for training and validating')\n",
    "print(\n",
    "    f'we have {test_labels.count(0)} cancer cells and {test_labels.count(1)} normal cells, {len(test_paths)} cells in total, for testing')\n",
    "\n",
    "# save the test image to target folders\n",
    "for i in range(len(test_paths)):\n",
    "    for j in range(len(dataset.samples)):\n",
    "        if dataset.samples[j][0] == test_paths[i] and dataset.samples[j][1] == 0:\n",
    "            utils.save_image(dataset[j][0],\n",
    "                             test_autoseg_cancer_path + test_paths[i].split('\\\\')[-1].split('.')[0] + \"_test.png\")\n",
    "            break\n",
    "        elif dataset.samples[j][0] == test_paths[i] and dataset.samples[j][1] == 1:\n",
    "            utils.save_image(dataset[j][0],\n",
    "                             test_autoseg_normal_path + test_paths[i].split('\\\\')[-1].split(\".\")[0] + \"_test.png\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:19:21.269079800Z",
     "start_time": "2025-08-09T10:19:12.221528400Z"
    }
   },
   "id": "e89b333ccbd9dfde",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.0157, 0.0309, 0.0269])\n",
      "Std: tensor([0.0377, 0.0673, 0.0844])\n"
     ]
    }
   ],
   "source": [
    "# train and validation dataset for calculating the image mean and std for normalization\n",
    "train_val_dataset = []\n",
    "for i in range(len(train_val_paths)):\n",
    "    for j in range(len(dataset.samples)):\n",
    "        if dataset.samples[j][0] == train_val_paths[i]:\n",
    "            train_val_dataset.append(dataset[j][0])\n",
    "            break\n",
    "\n",
    "images_mean, images_std = compute_mean_std(train_val_dataset, channels)\n",
    "print(\"Mean:\", images_mean)\n",
    "print(\"Std:\", images_std)\n",
    "\n",
    "# inverse_transform, to restore the images when saving them to whole folder and displaying them in XAI\n",
    "transform_inverse = v2.Compose([v2.Normalize(\n",
    "    mean=[-images_mean[0] / images_std[0], -images_mean[1] / images_std[1], -images_mean[2] / images_std[2]],\n",
    "    std=[1 / images_std[0], 1 / images_std[1], 1 / images_std[2]])])  # when mean = images_mean and std = images_std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:19:53.597864400Z",
     "start_time": "2025-08-09T10:19:33.353934200Z"
    }
   },
   "id": "35b52b7439993962",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save the validate image to target folders\n",
    "def save_validation_images(val_paths, dataset, cancer_path, normal_path):\n",
    "    for i in range(len(val_paths)):\n",
    "        for j in range(len(dataset.samples)):\n",
    "            if dataset.samples[j][0] == val_paths[i] and dataset.samples[j][1] == 0:\n",
    "                utils.save_image(dataset[j][0],\n",
    "                                 cancer_path + val_paths[i].split('\\\\')[-1].split('.')[0] + \"_val.png\")\n",
    "                break\n",
    "            elif dataset.samples[j][0] == val_paths[i] and dataset.samples[j][1] == 1:\n",
    "                utils.save_image(dataset[j][0],\n",
    "                                 normal_path + val_paths[i].split('\\\\')[-1].split(\".\")[0] + \"_val.png\")\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:20:10.040148100Z",
     "start_time": "2025-08-09T10:20:09.994333700Z"
    }
   },
   "id": "2f5183b33a533e8d",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_save_train_images(train_paths, dataset, cancer_path, normal_path):\n",
    "    # original training images\n",
    "    train_origin_cancer_dataset = []\n",
    "    train_origin_normal_dataset = []\n",
    "    for i in range(len(train_paths)):\n",
    "        for j in range(len(dataset.samples)):\n",
    "            if dataset.samples[j][0] == train_paths[i] and dataset.samples[j][1] == 0:\n",
    "                train_origin_cancer_dataset.append(\n",
    "                    {'image': dataset[j][0], 'filename': train_paths[i].split('\\\\')[-1].split('.')[0]})\n",
    "                break\n",
    "            elif dataset.samples[j][0] == train_paths[i] and dataset.samples[j][1] == 1:\n",
    "                train_origin_normal_dataset.append(\n",
    "                    {'image': dataset[j][0], 'filename': train_paths[i].split('\\\\')[-1].split('.')[0]})\n",
    "                break\n",
    "    \n",
    "    # Initialize an empty list to store the augmented images\n",
    "    augmented_images_class_0 = []\n",
    "    augmented_images_class_1 = []\n",
    "    \n",
    "    # Image augmentation\n",
    "    for image in train_origin_cancer_dataset:\n",
    "        for i in range(num_variations_per_image_0):\n",
    "            augmented_images_class_0.append({'image': transform_augmented(image['image']), 'filename': image['filename']})\n",
    "    \n",
    "    for image in train_origin_normal_dataset:\n",
    "        for i in range(num_variations_per_image_1):\n",
    "            augmented_images_class_1.append({'image': transform_augmented(image['image']), 'filename': image['filename']})\n",
    "    \n",
    "    # save training dataset (original + augmentation)\n",
    "    if num_variations_per_image_0 > 0:\n",
    "        for i in range(len(augmented_images_class_0)):\n",
    "            utils.save_image(augmented_images_class_0[i]['image'],\n",
    "                             cancer_path + str(int(i / num_variations_per_image_0)) + \"_\" + str(\n",
    "                                 i % num_variations_per_image_0) + \"_\" + augmented_images_class_0[i][\n",
    "                                 'filename'] + \"_aug.png\")\n",
    "    \n",
    "    if num_variations_per_image_1 > 0:\n",
    "        for i in range(len(augmented_images_class_1)):\n",
    "            utils.save_image(augmented_images_class_1[i]['image'],\n",
    "                             normal_path + str(int(i / num_variations_per_image_1)) + \"_\" + str(\n",
    "                                 i % num_variations_per_image_1) + \"_\" + augmented_images_class_1[i][\n",
    "                                 'filename'] + \"_aug.png\")\n",
    "    \n",
    "    # save original images (after resize)\n",
    "    for i in range(len(train_origin_cancer_dataset)):\n",
    "        utils.save_image(train_origin_cancer_dataset[i]['image'],\n",
    "                         cancer_path + str(i) + \"_\" + train_origin_cancer_dataset[i][\n",
    "                             'filename'] + \"_original.png\")\n",
    "    \n",
    "    for i in range(len(train_origin_normal_dataset)):\n",
    "        utils.save_image(train_origin_normal_dataset[i]['image'],\n",
    "                         normal_path + str(i) + \"_\" + train_origin_normal_dataset[i][\n",
    "                             'filename'] + \"_original.png\")\n",
    "    \n",
    "    print(\n",
    "        f\"we have generate {len(augmented_images_class_0)} augmented cancer cell images and {len(augmented_images_class_1)} augmented normal cell images.\")\n",
    "    print(\n",
    "        f'Totally we have {len(train_origin_cancer_dataset) + len(augmented_images_class_0)} cancer cell images and {len(train_origin_normal_dataset) + len(augmented_images_class_1)} normal cell images for training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:20:10.728840Z",
     "start_time": "2025-08-09T10:20:10.682555300Z"
    }
   },
   "id": "15a6634fa1b9d379",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_data(test_path, validation_path, train_path):\n",
    "    \n",
    "    # read test data\n",
    "    test_dataset = torchvision.datasets.ImageFolder(test_path, transform=transform_whole_dataset)\n",
    "    # read validation data\n",
    "    val_dataset = torchvision.datasets.ImageFolder(validation_path, transform=transform_whole_dataset)\n",
    "    # read train data\n",
    "    train_dataset = torchvision.datasets.ImageFolder(train_path, transform=transform_whole_dataset)\n",
    "    \n",
    "    return  test_dataset, val_dataset, train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:20:11.434998200Z",
     "start_time": "2025-08-09T10:20:11.372965300Z"
    }
   },
   "id": "1aaa73dc0a5a608c",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_whole_image(val_dataset, test_dataset, validation_whole_path, test_whole_path): \n",
    "    # save test and validate image (un-shuffle, easy to find which one is misclassified)\n",
    "    for i in range(len(val_dataset)):\n",
    "        utils.save_image(transform_inverse(val_dataset[i][0]),\n",
    "                         validation_whole_path + str(i) + \"_\" + str(val_dataset[i][1]) + \"_\" +\n",
    "                         val_dataset.samples[i][0].split('\\\\')[-1].split('.')[0] + \".png\")\n",
    "    for i in range(len(test_dataset)):\n",
    "        utils.save_image(transform_inverse(test_dataset[i][0]),\n",
    "                         test_whole_path + str(i) + \"_\" + str(test_dataset[i][1]) + \"_\" +\n",
    "                         test_dataset.samples[i][0].split('\\\\')[-1].split('.')[0] + \".png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:20:12.131124900Z",
     "start_time": "2025-08-09T10:20:12.074344100Z"
    }
   },
   "id": "3e32e00f62787577",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation openness is opened\n",
      "In fold 0\n",
      "we have 713 cancer cells and 134 normal cells, 847 cells in total, for training\n",
      "we have 179 cancer cells and 33 normal cells, 212 cells in total, for validating\n",
      "we have generate 713 augmented cancer cell images and 1072 augmented normal cell images.\n",
      "Totally we have 1426 cancer cell images and 1206 normal cell images for training\n",
      "In fold 1\n",
      "we have 713 cancer cells and 134 normal cells, 847 cells in total, for training\n",
      "we have 179 cancer cells and 33 normal cells, 212 cells in total, for validating\n",
      "we have generate 713 augmented cancer cell images and 1072 augmented normal cell images.\n",
      "Totally we have 1426 cancer cell images and 1206 normal cell images for training\n",
      "In fold 2\n",
      "we have 714 cancer cells and 133 normal cells, 847 cells in total, for training\n",
      "we have 178 cancer cells and 34 normal cells, 212 cells in total, for validating\n",
      "we have generate 714 augmented cancer cell images and 1064 augmented normal cell images.\n",
      "Totally we have 1428 cancer cell images and 1197 normal cell images for training\n",
      "In fold 3\n",
      "we have 714 cancer cells and 133 normal cells, 847 cells in total, for training\n",
      "we have 178 cancer cells and 34 normal cells, 212 cells in total, for validating\n",
      "we have generate 714 augmented cancer cell images and 1064 augmented normal cell images.\n",
      "Totally we have 1428 cancer cell images and 1197 normal cell images for training\n",
      "In fold 4\n",
      "we have 714 cancer cells and 134 normal cells, 848 cells in total, for training\n",
      "we have 178 cancer cells and 33 normal cells, 211 cells in total, for validating\n",
      "we have generate 714 augmented cancer cell images and 1072 augmented normal cell images.\n",
      "Totally we have 1428 cancer cell images and 1206 normal cell images for training\n"
     ]
    }
   ],
   "source": [
    "# This transform: to tensor and normalization is used for all images\n",
    "transform_whole_dataset = v2.Compose([v2.ToTensor(), v2.Normalize(mean=images_mean, std=images_std)])\n",
    "\n",
    "if not cross_validation:\n",
    "    # split train_val_dataset into train set and validation set\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_val_paths, train_val_labels, test_size=validation_percent / (1 - test_percent), stratify=train_val_labels,\n",
    "        random_state=seed) # here is a shuffle\n",
    "    \n",
    "    print(f'The cross validation openness is closed')\n",
    "    print(\n",
    "        f'we have {train_labels.count(0)} cancer cells and {train_labels.count(1)} normal cells, {len(train_paths)} cells in total, for training')\n",
    "    print(\n",
    "        f'we have {val_labels.count(0)} cancer cells and {val_labels.count(1)} normal cells, {len(val_paths)} cells in total, for validating')\n",
    "    \n",
    "    save_validation_images(val_paths, dataset, validation_autoseg_cancer_path, validation_autoseg_normal_path)\n",
    "    generate_save_train_images(train_paths, dataset, train_autoseg_cancer_path, train_autoseg_normal_path)\n",
    "    \n",
    "    # read test, train, validate data from target folders\n",
    "    test_dataset = []\n",
    "    val_dataset = []\n",
    "    train_dataset = []\n",
    "    test, val, train = read_data(test_autoseg_path, validation_autoseg_path, train_autoseg_path)\n",
    "    test_dataset.append(test)\n",
    "    val_dataset.append(val)\n",
    "    train_dataset.append(train)\n",
    "    \n",
    "    save_whole_image(val, test, validation_whole_autoseg_path, test_whole_autoseg_path)\n",
    "else:\n",
    "    print(f'The cross validation openness is opened')\n",
    "    test_dataset = []\n",
    "    val_dataset = []\n",
    "    train_dataset = []\n",
    "    skf = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed) # here is a shuffle\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_val_paths, train_val_labels)):\n",
    "        # use Stratified K-fold cross validation to create e.g. 10 folds, in each fold, the ratio between normal and cancer approximately keeping the same as the ratio in original image dataset.\n",
    "        train_paths = [train_val_paths[i] for i in train_idx]\n",
    "        train_labels = [train_val_labels[i] for i in train_idx]\n",
    "        \n",
    "        val_paths = [train_val_paths[i] for i in val_idx]\n",
    "        val_labels = [train_val_labels[i] for i in val_idx]\n",
    "        \n",
    "        print(f\"In fold {fold}\")\n",
    "        print(\n",
    "        f'we have {train_labels.count(0)} cancer cells and {train_labels.count(1)} normal cells, {len(train_paths)} cells in total, for training')\n",
    "        print(\n",
    "        f'we have {val_labels.count(0)} cancer cells and {val_labels.count(1)} normal cells, {len(val_paths)} cells in total, for validating')\n",
    "        \n",
    "        # new folders to save validation images for each fold.\n",
    "        validation_cv_fold_path = validation_autoseg_cv_path + \"cross validation \" + str(fold)\n",
    "        validation_cv_fold_split_path = validation_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\split'\n",
    "        validation_cv_fold_whole_path = validation_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\whole'\n",
    "        validation_cv_fold_split_cancer_path = validation_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\cancer'\n",
    "        validation_cv_fold_split_normal_path = validation_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\normal'\n",
    "        os.makedirs(validation_cv_fold_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_split_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_whole_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_split_cancer_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_split_normal_path, exist_ok=True)\n",
    "        \n",
    "        save_validation_images(val_paths, dataset, validation_cv_fold_split_cancer_path + '\\\\', validation_cv_fold_split_normal_path + '\\\\')\n",
    "        \n",
    "        # new folders to save train images for each fold.\n",
    "        train_cv_fold_path = train_autoseg_cv_path + \"cross validation \" + str(fold)\n",
    "        train_cv_fold_split_path = train_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\split'\n",
    "        train_cv_fold_whole_path = train_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\whole'\n",
    "        train_cv_fold_split_cancer_path = train_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\cancer'\n",
    "        train_cv_fold_split_normal_path = train_autoseg_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\normal'\n",
    "        os.makedirs(train_cv_fold_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_split_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_whole_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_split_cancer_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_split_normal_path, exist_ok=True)\n",
    "        \n",
    "        generate_save_train_images(train_paths, dataset, train_cv_fold_split_cancer_path + '\\\\', train_cv_fold_split_normal_path + '\\\\')\n",
    "        \n",
    "        # read test, train, validate data from target folders\n",
    "        test, val, train = read_data(test_autoseg_path, validation_cv_fold_split_path, train_cv_fold_split_path)\n",
    "        test_dataset.append(test)\n",
    "        val_dataset.append(val)\n",
    "        train_dataset.append(train)\n",
    "        \n",
    "        save_whole_image(val, test, validation_cv_fold_whole_path + '\\\\', test_whole_autoseg_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:30:22.034658700Z",
     "start_time": "2025-08-09T10:20:14.624019800Z"
    }
   },
   "id": "268b76021312d0b8",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a928af685e552f9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running config 1/1: {'dropout_p': 0.3, 'lr_backbone': 1e-05, 'lr_decay_epoch': 15, 'lr_head': 5e-05, 'unfrozen_blocks': [7, 8, 9, 10, 11], 'warmup_epoch': 3, 'weight_decay_backbone': 0.001, 'weight_decay_head': 0.0001}\n",
      "###############################################\n",
      "This is the fold: 0\n",
      "###############################################\n",
      "Train batch images shape: torch.Size([32, 3, 224, 224])\n",
      "Train batch labels: tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 1])\n",
      "Validate batch images shape: torch.Size([32, 3, 224, 224])\n",
      "Validate batch labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Test batch images shape: torch.Size([32, 3, 224, 224])\n",
      "Test batch labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Use device: cpu\n",
      "#trainable params: 71\n",
      "   base_model.cls_token\n",
      "   base_model.channel_token\n",
      "   base_model.pos_embed\n",
      "   base_model.token_learner.proj.weight\n",
      "   base_model.token_learner.proj.bias\n",
      "   base_model.blocks.7.self_attn.in_proj_weight\n",
      "   base_model.blocks.7.self_attn.in_proj_bias\n",
      "   base_model.blocks.7.self_attn.out_proj.weight\n",
      "   base_model.blocks.7.self_attn.out_proj.bias\n",
      "   base_model.blocks.7.linear1.weight\n",
      "   base_model.blocks.7.linear1.bias\n",
      "   base_model.blocks.7.linear2.weight\n",
      "   base_model.blocks.7.linear2.bias\n",
      "   base_model.blocks.7.norm1.weight\n",
      "   base_model.blocks.7.norm1.bias\n",
      "   base_model.blocks.7.norm2.weight\n",
      "   base_model.blocks.7.norm2.bias\n",
      "   base_model.blocks.8.self_attn.in_proj_weight\n",
      "   base_model.blocks.8.self_attn.in_proj_bias\n",
      "   base_model.blocks.8.self_attn.out_proj.weight\n",
      "   base_model.blocks.8.self_attn.out_proj.bias\n",
      "   base_model.blocks.8.linear1.weight\n",
      "   base_model.blocks.8.linear1.bias\n",
      "   base_model.blocks.8.linear2.weight\n",
      "   base_model.blocks.8.linear2.bias\n",
      "   base_model.blocks.8.norm1.weight\n",
      "   base_model.blocks.8.norm1.bias\n",
      "   base_model.blocks.8.norm2.weight\n",
      "   base_model.blocks.8.norm2.bias\n",
      "   base_model.blocks.9.self_attn.in_proj_weight\n",
      "   base_model.blocks.9.self_attn.in_proj_bias\n",
      "   base_model.blocks.9.self_attn.out_proj.weight\n",
      "   base_model.blocks.9.self_attn.out_proj.bias\n",
      "   base_model.blocks.9.linear1.weight\n",
      "   base_model.blocks.9.linear1.bias\n",
      "   base_model.blocks.9.linear2.weight\n",
      "   base_model.blocks.9.linear2.bias\n",
      "   base_model.blocks.9.norm1.weight\n",
      "   base_model.blocks.9.norm1.bias\n",
      "   base_model.blocks.9.norm2.weight\n",
      "   base_model.blocks.9.norm2.bias\n",
      "   base_model.blocks.10.self_attn.in_proj_weight\n",
      "   base_model.blocks.10.self_attn.in_proj_bias\n",
      "   base_model.blocks.10.self_attn.out_proj.weight\n",
      "   base_model.blocks.10.self_attn.out_proj.bias\n",
      "   base_model.blocks.10.linear1.weight\n",
      "   base_model.blocks.10.linear1.bias\n",
      "   base_model.blocks.10.linear2.weight\n",
      "   base_model.blocks.10.linear2.bias\n",
      "   base_model.blocks.10.norm1.weight\n",
      "   base_model.blocks.10.norm1.bias\n",
      "   base_model.blocks.10.norm2.weight\n",
      "   base_model.blocks.10.norm2.bias\n",
      "   base_model.blocks.11.self_attn.in_proj_weight\n",
      "   base_model.blocks.11.self_attn.in_proj_bias\n",
      "   base_model.blocks.11.self_attn.out_proj.weight\n",
      "   base_model.blocks.11.self_attn.out_proj.bias\n",
      "   base_model.blocks.11.linear1.weight\n",
      "   base_model.blocks.11.linear1.bias\n",
      "   base_model.blocks.11.linear2.weight\n",
      "   base_model.blocks.11.linear2.bias\n",
      "   base_model.blocks.11.norm1.weight\n",
      "   base_model.blocks.11.norm1.bias\n",
      "   base_model.blocks.11.norm2.weight\n",
      "   base_model.blocks.11.norm2.bias\n",
      "   base_model.norm.weight\n",
      "   base_model.norm.bias\n",
      "   head.0.weight\n",
      "   head.0.bias\n",
      "   head.3.weight\n",
      "   head.3.bias\n",
      "The learning rate of backbone is: 0.0, of head is 0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5906720256 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[53], line 350\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe learning rate of backbone is: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimizer\u001B[38;5;241m.\u001B[39mparam_groups[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, of head is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimizer\u001B[38;5;241m.\u001B[39mparam_groups[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 350\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m train(model, train_loader, criterion, optimizer, device, epoch, train_dataset[fold])\n\u001B[0;32m    351\u001B[0m     accuracy, wrong_predicted, val_loss, weighted_f1 \u001B[38;5;241m=\u001B[39m validate(model, val_loader, device, val_dataset[fold])\n\u001B[0;32m    352\u001B[0m     wrong_number\u001B[38;5;241m.\u001B[39mappend(wrong_predicted)\n",
      "Cell \u001B[1;32mIn[53], line 288\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, loader, criterion, optimizer, device, epoch, train_dataset)\u001B[0m\n\u001B[0;32m    286\u001B[0m X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    287\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# reset the grads\u001B[39;00m\n\u001B[1;32m--> 288\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(X, list_num_channels, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    289\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, targets)\n\u001B[0;32m    290\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# back propagation\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[53], line 131\u001B[0m, in \u001B[0;36mModifiedChadaViT.forward\u001B[1;34m(self, x, list_num_channels, index)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, list_num_channels, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;66;03m# the features from ViT backbone (batch size, 768 (dim of class token))\u001B[39;00m\n\u001B[1;32m--> 131\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model(x\u001B[38;5;241m=\u001B[39mx, index\u001B[38;5;241m=\u001B[39mindex, list_num_channels\u001B[38;5;241m=\u001B[39m[list_num_channels])\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;66;03m# pass the classification head\u001B[39;00m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(features)\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\chadavit\\chadavit\\src\\backbones\\vit\\chada_vit.py:322\u001B[0m, in \u001B[0;36mChAdaViT.forward\u001B[1;34m(self, x, index, list_num_channels)\u001B[0m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;66;03m# Apply the self-attention layers with masked self-attention\u001B[39;00m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[1;32m--> 322\u001B[0m     x \u001B[38;5;241m=\u001B[39m blk(x, src_key_padding_mask\u001B[38;5;241m=\u001B[39mchannel_mask)  \u001B[38;5;66;03m# Use src_key_padding_mask to mask out padded tokens\u001B[39;00m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;66;03m# 这个的输出也应该是（B,1960+1,D）\u001B[39;00m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;66;03m# Normalize\u001B[39;00m\n\u001B[0;32m    325\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x) \u001B[38;5;66;03m# 最后的输出会再做一次layernorm，这个的输出也应该是（B,1960+1,D）\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\chadavit\\chadavit\\src\\backbones\\vit\\chada_vit.py:93\u001B[0m, in \u001B[0;36mTransformerEncoderLayer.forward\u001B[1;34m(self, src, src_mask, src_key_padding_mask, return_attention)\u001B[0m\n\u001B[0;32m     91\u001B[0m x \u001B[38;5;241m=\u001B[39m src\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm_first: \u001B[38;5;66;03m# jiabang's change，我在init初始化函数中，我把这个改成了true\u001B[39;00m\n\u001B[1;32m---> 93\u001B[0m     attn, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sa_block(x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x), attn_mask \u001B[38;5;241m=\u001B[39m src_mask, key_padding_mask \u001B[38;5;241m=\u001B[39m src_key_padding_mask, return_attention \u001B[38;5;241m=\u001B[39m return_attention)\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_attention:\n\u001B[0;32m     95\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m attn_weights\n",
      "File \u001B[1;32mD:\\chadavit\\chadavit\\src\\backbones\\vit\\chada_vit.py:109\u001B[0m, in \u001B[0;36mTransformerEncoderLayer._sa_block\u001B[1;34m(self, x, attn_mask, key_padding_mask, return_attention)\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sa_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor, attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], return_attention: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 109\u001B[0m     x, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_attn(x, x, x,\n\u001B[0;32m    110\u001B[0m                        attn_mask\u001B[38;5;241m=\u001B[39mattn_mask,\n\u001B[0;32m    111\u001B[0m                        key_padding_mask\u001B[38;5;241m=\u001B[39mkey_padding_mask,\n\u001B[0;32m    112\u001B[0m                        need_weights\u001B[38;5;241m=\u001B[39mreturn_attention,\n\u001B[0;32m    113\u001B[0m                         average_attn_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x), attn_weights\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   1227\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1228\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1229\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1238\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   1239\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[0;32m   1240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1241\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1242\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1243\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[0;32m   1244\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias_k, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias_v, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_zero_attn,\n\u001B[0;32m   1245\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m   1246\u001B[0m         training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining,\n\u001B[0;32m   1247\u001B[0m         key_padding_mask\u001B[38;5;241m=\u001B[39mkey_padding_mask,\n\u001B[0;32m   1248\u001B[0m         need_weights\u001B[38;5;241m=\u001B[39mneed_weights,\n\u001B[0;32m   1249\u001B[0m         attn_mask\u001B[38;5;241m=\u001B[39mattn_mask,\n\u001B[0;32m   1250\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   1251\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[0;32m   1252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\functional.py:5476\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   5473\u001B[0m k \u001B[38;5;241m=\u001B[39m k\u001B[38;5;241m.\u001B[39mview(bsz, num_heads, src_len, head_dim)\n\u001B[0;32m   5474\u001B[0m v \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mview(bsz, num_heads, src_len, head_dim)\n\u001B[1;32m-> 5476\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001B[0;32m   5477\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(bsz \u001B[38;5;241m*\u001B[39m tgt_len, embed_dim)\n\u001B[0;32m   5479\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5906720256 bytes."
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import nn, optim\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "# hyper-parameters set for training the model\n",
    "# users can make the length of each list = 1 to close grid search\n",
    "param_grid = {\n",
    "'lr_backbone': [0.00001, 0.00005],\n",
    "'lr_head': [0.00005, 0.0001],\n",
    "'weight_decay_backbone': [0.001],\n",
    "'weight_decay_head': [0.0001],\n",
    "'dropout_p': [0.0, 0.3, 0.5],\n",
    "'warmup_epoch': [3, 6],\n",
    "'lr_decay_epoch': [12, 15],\n",
    "'unfrozen_blocks': [[7,8,9,10,11], [0,1,2,3,4,5,6,7,8,9,10,11]]\n",
    "}\n",
    "'''\n",
    "'''\n",
    "param_grid = {\n",
    "'lr_backbone': [0.00001, 0.00005],\n",
    "'lr_head': [0.00005],\n",
    "'weight_decay_backbone': [0.001],\n",
    "'weight_decay_head': [0.0001],\n",
    "'dropout_p': [0.3],\n",
    "'warmup_epoch': [3],\n",
    "'lr_decay_epoch': [12, 15],\n",
    "'unfrozen_blocks': [[7,8,9,10,11]]\n",
    "}   \n",
    "'''\n",
    "param_grid = {\n",
    "'lr_backbone': [0.00001],\n",
    "'lr_head': [0.00005],\n",
    "'weight_decay_backbone': [0.001],\n",
    "'weight_decay_head': [0.0001],\n",
    "'dropout_p': [0.3],\n",
    "'warmup_epoch': [3],\n",
    "'lr_decay_epoch': [15],\n",
    "'unfrozen_blocks': [[7,8,9,10,11]]\n",
    "}    \n",
    "\n",
    "grid = list(ParameterGrid(param_grid)) # generate all the hyper-parameter combination\n",
    "gird_search_result = [] # store the cross-validation performance of each hyper-parameter set\n",
    "for set_num, hyper_params in enumerate(grid):\n",
    "    print(f\"Running config {set_num + 1}/{len(grid)}: {hyper_params}\")\n",
    "    \n",
    "    config = {\n",
    "    'lr_backbone': hyper_params['lr_backbone'],\n",
    "    'lr_head': hyper_params['lr_head'],\n",
    "    'weight_decay_backbone': hyper_params['weight_decay_backbone'],\n",
    "    'weight_decay_head': hyper_params['weight_decay_head'],\n",
    "    'dropout_p': hyper_params['dropout_p'],\n",
    "    'num_epochs': 30,\n",
    "    'warmup_epoch': hyper_params['warmup_epoch'],\n",
    "    'lr_decay_epoch': hyper_params['lr_decay_epoch'],\n",
    "    'unfrozen_blocks': hyper_params['unfrozen_blocks']\n",
    "    } # configure all the hyper-parameters, including not for grid search\n",
    "\n",
    "    for fold in range(fold_num):\n",
    "        print('###############################################')\n",
    "        print(f'This is the fold: {fold}')\n",
    "        print('###############################################')\n",
    "        # Load the train, validate, and test dataset\n",
    "        #torch.manual_seed(seed) # if the shuffle is displayed different each time, use torch.manual_seed(seed)\n",
    "        train_loader = DataLoader(train_dataset[fold], batch_size=batch_size, shuffle=True)  # here is a shuffle\n",
    "        val_loader = DataLoader(val_dataset[fold], batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset[fold], batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            print(f\"Train batch images shape: {images.shape}\")\n",
    "            print(f\"Train batch labels: {labels}\")\n",
    "            break\n",
    "        for images, labels in val_loader:\n",
    "            print(f\"Validate batch images shape: {images.shape}\")\n",
    "            print(f\"Validate batch labels: {labels}\")\n",
    "            break\n",
    "        for images, labels in test_loader:\n",
    "            print(f\"Test batch images shape: {images.shape}\")\n",
    "            print(f\"Test batch labels: {labels}\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        # Set gpu/cpu\n",
    "        print(f\"Use device: {device}\")\n",
    "        \n",
    "        # Model Params\n",
    "        PATCH_SIZE = 16\n",
    "        EMBED_DIM = 192\n",
    "        RETURN_ALL_TOKENS = False\n",
    "        MAX_NUMBER_CHANNELS = 10\n",
    "        \n",
    "        # use chadavit model\n",
    "        model = ChAdaViT(\n",
    "            patch_size=PATCH_SIZE,\n",
    "            embed_dim=EMBED_DIM,\n",
    "            return_all_tokens=RETURN_ALL_TOKENS,\n",
    "            max_number_channels=MAX_NUMBER_CHANNELS,\n",
    "        )\n",
    "        \n",
    "        assert (\n",
    "            CKPT_PATH.endswith(\".ckpt\")\n",
    "            or CKPT_PATH.endswith(\".pth\")\n",
    "            or CKPT_PATH.endswith(\".pt\")\n",
    "        ) # ensure the CKPT_PATH ends correctly\n",
    "        state = torch.load(CKPT_PATH, map_location=\"cpu\")[\"state_dict\"]\n",
    "        for k in list(state.keys()):\n",
    "            if \"encoder\" in k:\n",
    "                state[k.replace(\"encoder\", \"backbone\")] = state[k]\n",
    "            if \"backbone\" in k:\n",
    "                state[k.replace(\"backbone.\", \"\")] = state[k]\n",
    "            del state[k]\n",
    "        model.load_state_dict(state, strict=False) # load the pre-trained parameter\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        class ModifiedChadaViT(nn.Module):\n",
    "            def __init__(self, base_model):\n",
    "                super(ModifiedChadaViT, self).__init__()\n",
    "                self.base_model = base_model\n",
    "                self.head = nn.Sequential(nn.Linear(base_model.embed_dim, 768),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Dropout(config['dropout_p']),\n",
    "                                          nn.Linear(768, 2))  # modify the head from Identify to 2-class classification (Linear Layer) and add drop out (included in grid search)\n",
    "        \n",
    "            def forward(self, x, list_num_channels, index=0):\n",
    "                # the features from ViT backbone (batch size, 768 (dim of class token))\n",
    "                features = self.base_model(x=x, index=index, list_num_channels=[list_num_channels])\n",
    "                # pass the classification head\n",
    "                return self.head(features)\n",
    "        \n",
    "        \n",
    "        model = ModifiedChadaViT(model).to(device)  # TODO: use the basic ViT-B-16 model, see the most below\n",
    "        model.mixed_channels = mixed_channels # all of the inputs share the same channel number.\n",
    "        \n",
    "        for param in model.base_model.blocks.parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer_index in config['unfrozen_blocks']:\n",
    "            layer = model.base_model.blocks[layer_index]  \n",
    "            # only un-froze the last few layers (included in grid search)\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        for param in model.head.parameters(): # un froze the classification head\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        class EarlyStopping:\n",
    "            def __init__(self, patience=5, mode='loss', min_delta=0.0, fold=0, epoch_number = 10):\n",
    "                # min_delta used to measure the extension of loss decrease, only the loss decrease > min_delta, we can say early stop check pass.\n",
    "                self.patience = patience\n",
    "                self.mode = mode\n",
    "                self.fold = fold\n",
    "                self.min_delta = min_delta\n",
    "                self.counter = 0\n",
    "                self.epoch_number = epoch_number\n",
    "                self.best = None\n",
    "                self.early_stop = False\n",
    "        \n",
    "            def __call__(self, val_loss, accuracy, epoch):\n",
    "                print(\"the mode of early stopping is \" + self.mode)\n",
    "                if self.mode == 'loss':\n",
    "                    if self.best is None:\n",
    "                        print(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = val_loss\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best val_loss is: {self.best}\")\n",
    "                    elif val_loss < self.best - self.min_delta:\n",
    "                        print(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = val_loss\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best val_loss is: {self.best}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        print(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        print(f\"The best val_loss is still: {self.best}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "                elif self.mode == 'accuracy':\n",
    "                    if self.best is None:\n",
    "                        print(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = accuracy\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best accuracy / weighted f1 score is: {self.best}\")\n",
    "                    elif accuracy > self.best:\n",
    "                        print(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = accuracy\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best accuracy / weighted f1 score is: {self.best}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        print(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        print(f\"The best accuracy / weighted f1 score is still: {self.best}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "                elif self.mode == 'loss and accuracy':\n",
    "                    if self.best is None:\n",
    "                        print(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    elif val_loss < self.best[0] - self.min_delta and accuracy > self.best[1]:\n",
    "                        print(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        print(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        print(f\"The best val_loss and accuracy / weighted f1 score are still: {self.best[0]}, {self.best[1]}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "                elif self.mode == 'loss or accuracy':\n",
    "                    if self.best is None:\n",
    "                        print(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    elif val_loss < self.best[0] - self.min_delta or accuracy > self.best[1]:\n",
    "                        print(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        print(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        print(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        print(f\"The best val_loss and accuracy / weighted f1 score are still: {self.best[0]}, {self.best[1]}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "        \n",
    "        num_epochs = config['num_epochs']\n",
    "        warm_up_epochs = config['warmup_epoch']\n",
    "        total_steps = (config['warmup_epoch'] + config['lr_decay_epoch']) * len(train_loader)\n",
    "        warm_up_steps = warm_up_epochs * len(train_loader)\n",
    "        # Set L.F., optimizer\n",
    "        #weights = torch.tensor([1.0, 1.5]).to(device) # set higher weights for positive class in CEL\n",
    "        #criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW([{'params': filter(lambda p: p.requires_grad, model.base_model.parameters()), 'weight_decay': config['weight_decay_backbone'], 'lr': config['lr_backbone']}, {'params': model.head.parameters(), 'weight_decay': config['weight_decay_head'], 'lr': config['lr_head']}])  # maybe can use AdamW for better combination with weight decay\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        # weight_decay is not always good, see notebook 55 for detail\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warm_up_steps, num_training_steps=total_steps)  # the learning rate will warm-up firstly, then cosine decay to 0\n",
    "        \n",
    "        \n",
    "        # TODO: the normalization need to be mean = [0.485, 0.456, 0.406] std  = [0.229, 0.224, 0.225]. ------ not good, use the image_mean and image_std will let the model converge better.\n",
    "        # TODO: Set an Early stop and try epoch = 10 or more. ------ Done\n",
    "        # TODO: Try another kind of lr-scheduler, lr value or let the gamma smaller, maybe 0.25\n",
    "        # TODO: In optimizer, use momentum? ------ Adam and AdamW don't need momentum, they are carried with momentum. Only SGD needs momentum, but prefer to using AdamW in ViT training\n",
    "        # TODO: Use another optimizer? AdamW, SGD, etc. ------ Done\n",
    "        # TODO: Use weight decay? dropout in the classifier ? ------ Done\n",
    "        # TODO: Use warm-up + learning rate scheduler combination? ------ Done\n",
    "        # TODO: Change the size of Batch?\n",
    "        # TODO: Use hyperparameter search algorithm, like random search, grid search, etc. ------ Done\n",
    "        # TODO: Use TensorBoard to monitor the training process\n",
    "        # TODO: Use other kinds of augmented methods\n",
    "        # TODO: add label smoothing for small dataset and focal loss for hard to classify sample\n",
    "        # TODO: Freeze fewer blocks, and un-freeze position embedding, layernorm ------ Done\n",
    "        # TODO: Try hybrid ViT, Swin-ViT (both have CNN’s properties) or Dei-T (better on small dataset)\n",
    "        # TODO: 再看几篇paper的experiment，看看别人是怎么做的\n",
    "        # TODO: ViT should be put into comparison model set\n",
    "        \n",
    "        # train func\n",
    "        def train(model, loader, criterion, optimizer, device, epoch, train_dataset):\n",
    "            model.train()  # set model to train mode\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(loader, 0):  # each batch input\n",
    "                inputs, targets = data\n",
    "                targets = targets.to(device)\n",
    "                inputs = collate_images(inputs) # collate the input image batch to a sequence of channel\n",
    "                X, list_num_channels = inputs\n",
    "                X = X.to(device)\n",
    "                optimizer.zero_grad()  # reset the grads\n",
    "                outputs = model(X, list_num_channels, index=0)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()  # back propagation\n",
    "                optimizer.step()  # update parameter\n",
    "                running_loss += loss.item() * targets.size(0)\n",
    "                print(\"In epoch \" + str(epoch + 1) + \", batch: \" + str(i + 1) + \", average loss per image: \" + str(\n",
    "                    loss.item()))\n",
    "        \n",
    "                correct_train = 0\n",
    "                _, predicted_train = outputs.max(1)  # model predicted class\n",
    "                correct_train += (predicted_train == targets).sum().item()\n",
    "                print(\"Accuracy of the network on the train set: \" + str(correct_train / targets.size(0)))\n",
    "                \n",
    "                scheduler.step()  # regularize the learning rate\n",
    "        \n",
    "            return running_loss / len(train_dataset)\n",
    "        \n",
    "        \n",
    "        # validate func\n",
    "        def validate(model, loader, device, val_dataset):\n",
    "            model.eval()  # set model to validate mode\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            false = []\n",
    "            running_loss = 0.0\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(loader, 0):\n",
    "                    inputs, targets = data\n",
    "                    targets = targets.to(device)\n",
    "                    inputs = collate_images(inputs) # collate the input image batch to a sequence of channel\n",
    "                    X, list_num_channels = inputs\n",
    "                    X = X.to(device)\n",
    "                    outputs = model(X, list_num_channels, index=0)\n",
    "                    _, predicted = outputs.max(1)  # model predicted class\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "                    total += targets.size(0)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    running_loss += loss.item() * targets.size(0) # the sum of val_loss in one batch            \n",
    "                    \n",
    "                    for target in targets:\n",
    "                        y_true.append(target.item())\n",
    "                    for predict in predicted:\n",
    "                        y_pred.append(predict.item())\n",
    "                    \n",
    "                    for result in range(len(predicted)): # collect wrong samples\n",
    "                        if predicted[result] != targets[result]:\n",
    "                            false.append(i * batch_size + result)\n",
    "                    \n",
    "                y_true = np.array(y_true)\n",
    "                y_pred = np.array(y_pred)\n",
    "                \n",
    "            return correct / total, false, running_loss / len(val_dataset), f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        \n",
    "        # train and validate\n",
    "        early_stopping = EarlyStopping(patience=5, mode=early_stop_mode, fold=fold, epoch_number = num_epochs)\n",
    "        wrong_number = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"The learning rate of backbone is: {optimizer.param_groups[0]['lr']}, of head is {optimizer.param_groups[1]['lr']}\")\n",
    "            train_loss = train(model, train_loader, criterion, optimizer, device, epoch, train_dataset[fold])\n",
    "            accuracy, wrong_predicted, val_loss, weighted_f1 = validate(model, val_loader, device, val_dataset[fold])\n",
    "            wrong_number.append(wrong_predicted)\n",
    "            print(\"====================\" + str(epoch + 1) + \"====================\")\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print(f'Average train loss per image: {train_loss:.7f}')\n",
    "            print(f'Average validate loss per image: {val_loss:.7f}')\n",
    "            print(f'Validate accuracy: {accuracy:.4f}')\n",
    "            print(\"====================\" + str(epoch + 1) + \"====================\")\n",
    "        \n",
    "            early_stopping(val_loss, weighted_f1, epoch)\n",
    "        \n",
    "            if early_stopping.early_stop:\n",
    "                print(\" 🔥 Early stopping, Stop Training\")\n",
    "                print(f\"select the epoch: {epoch - early_stopping.counter + 1}\")\n",
    "                for wrong_result in wrong_number[epoch - early_stopping.counter]:\n",
    "                    print(\"The number \" + str(wrong_result) + \" is wrong!\")\n",
    "                break\n",
    "    \n",
    "            if epoch == num_epochs - 1:\n",
    "                print(\"train until the last epoch!\")\n",
    "                for wrong_result in wrong_predicted:\n",
    "                    print(\"The number \" + str(wrong_result) + \" is wrong!\")\n",
    "                    \n",
    "    model_accuracy = []\n",
    "    model_recall = []\n",
    "    model_precision = []\n",
    "    model_auc_roc_macro = []\n",
    "    model_auc_roc_micro = []\n",
    "    model_auc_roc_weighted = []\n",
    "    model_f1_macro = []\n",
    "    model_f1_micro = []\n",
    "    model_f1_weighted = []\n",
    "    \n",
    "    \n",
    "    for fold in range(fold_num):\n",
    "        model.load_state_dict(torch.load(\"best_model_parameter_\" + str(fold) + \".pth\"))\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_prob = []\n",
    "        \n",
    "        for data in range(len(val_dataset[fold])):\n",
    "            y_true.append(val_dataset[fold][data][1])\n",
    "        \n",
    "            inputs = val_dataset[fold][data][0]\n",
    "            inputs = inputs.to(device)\n",
    "            input_tensor = collate_images(inputs.unsqueeze(0))\n",
    "            X, list_num_channels = input_tensor\n",
    "            X = X.to(device)\n",
    "            outputs = model(X, list_num_channels, index=0)\n",
    "            predicted_class = outputs.argmax(dim=1).item()\n",
    "            y_pred.append(predicted_class)\n",
    "        \n",
    "            prob = torch.softmax(outputs, dim=1)[:, 1].item()\n",
    "            y_prob.append(prob)\n",
    "        \n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_prob = np.array(y_prob)\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        auc_roc_macro = roc_auc_score(y_true, y_prob)\n",
    "        auc_roc_micro = roc_auc_score(y_true, y_prob)\n",
    "        auc_roc_weighted = roc_auc_score(y_true, y_prob)\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        accuracy = 0.0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == y_true[i]:\n",
    "                accuracy += 1.0\n",
    "        model_accuracy.append(accuracy/len(y_pred))\n",
    "        model_recall.append(recall)\n",
    "        model_precision.append(precision)\n",
    "        model_auc_roc_macro.append(auc_roc_macro)\n",
    "        model_auc_roc_micro.append(auc_roc_micro)\n",
    "        model_auc_roc_weighted.append(auc_roc_weighted)\n",
    "        model_f1_macro.append(f1_macro)\n",
    "        model_f1_micro.append(f1_micro)\n",
    "        model_f1_weighted.append(f1_weighted)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy/len(y_pred):.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"AUC-ROC Macro: {auc_roc_macro:.4f}\")\n",
    "        print(f\"AUC-ROC Micro: {auc_roc_micro:.4f}\")\n",
    "        print(f\"AUC-ROC Weighted: {auc_roc_weighted:.4f}\")\n",
    "        print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "        print(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "        print(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "        \n",
    "    print(f\"The mean and std of accuracy are: {np.array(model_accuracy).mean()}, and {np.array(model_accuracy).std()}\")\n",
    "    print(f\"The mean and std of recall are: {np.array(model_recall).mean()}, and {np.array(model_recall).std()}\")\n",
    "    print(f\"The mean and std of precision are: {np.array(model_precision).mean()}, and {np.array(model_precision).std()}\")\n",
    "    print(f\"The mean and std of auc_roc_macro are: {np.array(model_auc_roc_macro).mean()}, and {np.array(model_auc_roc_macro).std()}\")\n",
    "    print(f\"The mean and std of auc_roc_micro are: {np.array(model_auc_roc_micro).mean()}, and {np.array(model_auc_roc_micro).std()}\")\n",
    "    print(f\"The mean and std of auc_roc_weighted are: {np.array(model_auc_roc_weighted).mean()}, and {np.array(model_auc_roc_weighted).std()}\")\n",
    "    print(f\"The mean and std of f1_macro are: {np.array(model_f1_macro).mean()}, and {np.array(model_f1_macro).std()}\")\n",
    "    print(f\"The mean and std of f1_micro are: {np.array(model_f1_micro).mean()}, and {np.array(model_f1_micro).std()}\")\n",
    "    print(f\"The mean and std of f1_weighted are: {np.array(model_f1_weighted).mean()}, and {np.array(model_f1_weighted).std()}\")\n",
    "    \n",
    "    gird_search_result.append({'hyperparams': hyper_params,\n",
    "                               'accuracy mean':np.array(model_accuracy).mean(),\n",
    "                               'accuracy std': np.array(model_accuracy).std(),\n",
    "                               'recall mean': np.array(model_recall).mean(),\n",
    "                               'recall std': np.array(model_recall).std(),\n",
    "                               'precision mean': np.array(model_precision).mean(),\n",
    "                               'precision std': np.array(model_precision).std(),\n",
    "                               'auc_roc_macro mean': np.array(model_auc_roc_macro).mean(),\n",
    "                               'auc_roc_macro std': np.array(model_auc_roc_macro).std(),\n",
    "                               'auc_roc_micro mean': np.array(model_auc_roc_micro).mean(),\n",
    "                               'auc_roc_micro std': np.array(model_auc_roc_micro).std(),\n",
    "                               'auc_roc_weighted mean': np.array(model_auc_roc_weighted).mean(),\n",
    "                               'auc_roc_weighted std': np.array(model_auc_roc_weighted).std(),\n",
    "                               'f1_macro mean': np.array(model_f1_macro).mean(),\n",
    "                               'f1_macro std': np.array(model_f1_macro).std(),\n",
    "                               'f1_micro mean': np.array(model_f1_micro).mean(),\n",
    "                               'f1_micro std': np.array(model_f1_micro).std(),\n",
    "                               'f1_weighted mean': np.array(model_f1_weighted).mean(),\n",
    "                               'f1_weighted std': np.array(model_f1_weighted).std()})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T15:02:22.584666800Z",
     "start_time": "2025-08-09T15:01:09.698880700Z"
    }
   },
   "id": "fe7100fa0f8a821",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb119ceaa26a43be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
