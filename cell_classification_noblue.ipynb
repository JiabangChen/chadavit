{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:15:27.997728400Z",
     "start_time": "2025-08-11T16:15:27.948703800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_grad_cam\n",
    "import torch.hub as hub\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.backbones.vit.chada_vit import ChAdaViT\n",
    "import hashlib\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "channels = 2 # without blue channel\n",
    "batch_size = 32\n",
    "baseline = False # whether to use baseline model: VGG-16, DenseNet-121,ResNet-50, efficientnet_v2_small, convnext_base\n",
    "baseline_model = ['densenet121', 'efficientnet_v2_s', 'resnet50', 'vgg16', 'convnext_base']\n",
    "early_stop_mode = 'accuracy'  # choose 'loss' mode, 'accuracy' mode, 'loss or accuracy' mode or 'loss and accuracy' mode, here using weighted F1-score to replace accuracy, especially in imbalanced dataset\n",
    "# Number of variations to generate per image\n",
    "num_variations_per_image_0 = 1\n",
    "num_variations_per_image_1 = 8\n",
    "test_percent = 0.15  # choose the proportion size of test set\n",
    "validation_percent = 0.1  # choose the proportion size of validation set if close the cross validation\n",
    "cross_validation = True  # choose open or close Cross-Validation\n",
    "fold_num = 5  # choose the number of fold if open the cross-validation\n",
    "run_name = 'channel_wise_dino_without_nucleus'\n",
    "if not cross_validation:\n",
    "    fold_num = 1\n",
    "CKPT_PATH = \"weights.ckpt\"\n",
    "mixed_channels = False\n",
    "    \n",
    "dataset_autoseg_noblue_path = \"D:\\\\cell_autoseg_noblue\\\\red_green\"\n",
    "train_autoseg_noblue_path = \"D:\\\\cell_autoseg_train_noblue\\split\"\n",
    "train_autoseg_noblue_cancer_path = 'D:\\\\cell_autoseg_train_noblue\\split\\\\cancer\\\\'\n",
    "train_autoseg_noblue_normal_path = 'D:\\\\cell_autoseg_train_noblue\\split\\\\normal\\\\'\n",
    "train_autoseg_noblue_cv_path = 'D:\\\\cell_autoseg_train_noblue\\\\cross validation\\\\'\n",
    "test_autoseg_noblue_path = 'D:\\\\cell_autoseg_test_noblue\\split'\n",
    "test_whole_autoseg_noblue_path = 'D:\\\\cell_autoseg_test_noblue\\whole\\\\'\n",
    "test_autoseg_noblue_cancer_path = \"D:\\\\cell_autoseg_test_noblue\\split\\\\cancer\\\\\"\n",
    "test_autoseg_noblue_normal_path = 'D:\\\\cell_autoseg_test_noblue\\split\\\\normal\\\\'\n",
    "validation_autoseg_noblue_path = 'D:\\\\cell_autoseg_validation_noblue\\split'\n",
    "validation_whole_autoseg_noblue_path = 'D:\\\\cell_autoseg_validation_noblue\\whole\\\\'\n",
    "validation_autoseg_noblue_cancer_path = 'D:\\\\cell_autoseg_validation_noblue\\split\\\\cancer\\\\'\n",
    "validation_autoseg_noblue_normal_path = 'D:\\\\cell_autoseg_validation_noblue\\split\\\\normal\\\\'\n",
    "validation_autoseg_noblue_cv_path = 'D:\\\\cell_autoseg_validation_noblue\\\\cross validation\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_hash(file_path, expected_hash):\n",
    "    md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        while chunk := f.read(4096):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest() == expected_hash"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:00:08.704173200Z",
     "start_time": "2025-08-11T16:00:08.649565500Z"
    }
   },
   "id": "dbca239cf5b4e77c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_hash(CKPT_PATH, \"e8a24ac58b8e34bdce10e0024d507f2e\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:00:13.669707100Z",
     "start_time": "2025-08-11T16:00:09.506673800Z"
    }
   },
   "id": "6f1242dd349af061",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def collate_images(batch: list):\n",
    "    \"\"\"\n",
    "    Collate a batch of images into a list of channels and a mapping of the number of channels per image.\n",
    "    But here, we will remove the blue channel before entering into the chada_vit!\n",
    "    Args:\n",
    "        batch (list): A batch of images Tensor(B,C,H,W)\n",
    "\n",
    "    Return:\n",
    "        channels_list (torch.Tensor): A tensor of shape (X*num_channels, 1, height, width)\n",
    "        num_channels_list (list): A list of the number of channels per image\n",
    "    \"\"\"\n",
    "    num_channels_list = []\n",
    "    channels_list = []\n",
    "\n",
    "    # Iterate over the list of images and extract the channels\n",
    "    for image in batch: \n",
    "        num_channels = image.shape[0] - 1 # discard the blue channel num_channels = 2\n",
    "        num_channels_list.append(num_channels) \n",
    "\n",
    "        for channel in range(num_channels): # discard the blue channel\n",
    "            channel_image = image[channel, :, :].unsqueeze(0) \n",
    "            channels_list.append(channel_image)\n",
    "\n",
    "    channels_list = torch.cat(channels_list, dim=0).unsqueeze(\n",
    "        1\n",
    "    )  # Shape: (X*num_channels, 1, height, width)\n",
    "\n",
    "    return channels_list, num_channels_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:02:26.508447300Z",
     "start_time": "2025-08-11T16:02:26.457964300Z"
    }
   },
   "id": "a22927a8ec6b1c60",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "SCRIPT_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "log_file = SCRIPT_DIR / f\"{run_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logger = logging.getLogger(run_name)\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "for h in logger.handlers[:]:\n",
    "    try:\n",
    "        h.flush()\n",
    "    except Exception:\n",
    "        pass\n",
    "    h.close()\n",
    "    logger.removeHandler(h)\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(fmt)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "fh = logging.FileHandler(log_file, mode='a', encoding=\"utf-8\")\n",
    "fh.setFormatter(fmt)\n",
    "logger.addHandler(fh)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9998a8afe15dee60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_lr_curve(lr_per_step, title=\"Learning Rate vs. Step\", fold_number = None, hyper_setnum = 0, model_type = \"DINO\"):\n",
    "    \"\"\"\n",
    "    lr_per_step: list/array，length = the number of step\n",
    "    \"\"\"\n",
    "    assert fold_number is not None, \"need to input fold number!\"\n",
    "    steps = range(len(lr_per_step))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, lr_per_step, label=\"LR\", marker = 'o', markevery = 1)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(title + \"_\" + model_type + \"_\" + str(hyper_setnum) + \"_\" + str(fold_number) + \".png\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_curves(train_loss_per_epoch, val_loss_per_epoch, title=\"Loss vs. Epoch\", fold_number = None, hyper_setnum = 0, model_type = \"DINO\"):\n",
    "    \"\"\"\n",
    "    train_loss_per_epoch: list/array，each object is the average training loss in an epoch \n",
    "    val_loss_per_epoch:   list/array，each object is the average validating loss in an epoch \n",
    "    \"\"\"\n",
    "    assert fold_number is not None, \"need to input fold number!\"\n",
    "    assert len(train_loss_per_epoch) == len(val_loss_per_epoch), \\\n",
    "        f\"the length of training loss and validating loss are different: {len(train_loss_per_epoch)} vs {len(val_loss_per_epoch)}!\"\n",
    "\n",
    "    epochs = range(1, len(train_loss_per_epoch) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss_per_epoch, label=\"Train Loss\", marker = 'o', markevery = 1)\n",
    "    plt.plot(epochs, val_loss_per_epoch, label=\"Val Loss\", marker = 's', markevery = 1)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(title + \"_\" + model_type + \"_\" + str(hyper_setnum) + \"_\" + str(fold_number) + \".png\", bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710e676db326b72b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "def get_cosine_with_warmup_tail(optimizer, num_warmup_steps, num_training_steps, min_lr_factor=0.1, num_cycles=0.5):\n",
    "    \"\"\"\n",
    "    Cosine decay with warmup and a fixed min_lr_factor (tail factor).\n",
    "    min_lr_factor = lr_min / lr_max\n",
    "    This is used to prevent the lr from decaying to 0\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        # warmup\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * num_cycles * 2.0 * progress))\n",
    "        # 尾部留一个 min_lr_factor\n",
    "        return min_lr_factor + (1 - min_lr_factor) * cosine_decay\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2406bed0be7df91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_mean_std(ds, channels):  # calculate the mean and std for each channel in the dataset\n",
    "    mean = torch.zeros(channels)\n",
    "    std = torch.zeros(channels)\n",
    "    for image in ds:\n",
    "        for channel in range(channels):\n",
    "            mean[channel] += image[channel, :, :].mean()\n",
    "            std[channel] += image[channel, :, :].std()\n",
    "    mean = mean / len(ds)\n",
    "    std = std / len(ds) #TODO: is this a correct way to calculate STD?\n",
    "    \n",
    "    mean_blue = torch.zeros(1) # add the mean = 0 for blue channel\n",
    "    std_blue = torch.ones(1) # std = 1, so the blue channel will not be altered by Normalization\n",
    "    mean = torch.cat([mean, mean_blue])\n",
    "    std = torch.cat([std, std_blue])\n",
    "    \n",
    "    return mean, std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:02:29.377368200Z",
     "start_time": "2025-08-11T16:02:29.342312500Z"
    }
   },
   "id": "c2f0bb12c6501248",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ResizeWithPadding:\n",
    "    def __init__(self, size, fill=0):\n",
    "        \"\"\"\n",
    "        size: tuple (width, height) target size\n",
    "        fill: pixel value to fill\n",
    "        \"\"\"\n",
    "        self.target_width, self.target_height = size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # obtain the size of original image\n",
    "        orig_width, orig_height = img.size\n",
    "\n",
    "        # use the smaller ratio to scale the width and height equally.\n",
    "        width_ratio = self.target_width / orig_width\n",
    "        height_ratio = self.target_height / orig_height\n",
    "        if width_ratio <= height_ratio:\n",
    "            new_width = int(orig_width * width_ratio + 0.1) # plus 0.1 to prevent the float error\n",
    "            new_height = int(orig_height * width_ratio)\n",
    "        else:\n",
    "            new_width = int(orig_width * height_ratio)\n",
    "            new_height = int(orig_height * height_ratio + 0.1)\n",
    "\n",
    "        # resize\n",
    "        img = F.resize(img, [new_height, new_width])  # the resize in F needs the format of input as (height, width)\n",
    "\n",
    "        # calculate padding size\n",
    "        pad_left = (self.target_width - new_width) // 2\n",
    "        pad_top = (self.target_height - new_height) // 2\n",
    "        pad_right = self.target_width - new_width - pad_left\n",
    "        pad_bottom = self.target_height - new_height - pad_top\n",
    "\n",
    "        # 添加 padding\n",
    "        img = F.pad(img, [pad_left, pad_top, pad_right, pad_bottom], fill=self.fill)\n",
    "        \n",
    "        assert img.size[0] == self.target_width and img.size[1] == self.target_height, 'Output Image size is incorrect!'\n",
    "\n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:02:30.599102100Z",
     "start_time": "2025-08-11T16:02:30.554582700Z"
    }
   },
   "id": "3e2af519ac5b2e3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# Define the path to the directory containing images\n",
    "transform = v2.Compose([ResizeWithPadding((224,224)), v2.ToTensor()])\n",
    "# resize and transfer to tensor\n",
    "dataset = torchvision.datasets.ImageFolder(dataset_autoseg_noblue_path, transform=transform)  # read data\n",
    "# Assuming images are organized in subdirectories where each subdirectory name is the class label\n",
    "# 0 is cancer, 1 is normal\n",
    "\n",
    "transform_augmented = v2.Compose([v2.RandomHorizontalFlip(),\n",
    "                                  v2.RandomVerticalFlip(),\n",
    "                                  v2.RandomRotation(degrees=40),\n",
    "                                  v2.RandomAffine(degrees=40, translate=(0.1, 0.1), shear=(-8,8,-8,8), scale=(0.9, 1.1)),\n",
    "                                  #v2.RandomErasing(p=0.2,scale=(0.02,0.05),ratio=(0.3,3)),\n",
    "                                  #v2.RandomResizedCrop(224, scale=(0.8, 1.0), interpolation=InterpolationMode.BICUBIC, antialias=True),\n",
    "                                  #v2.RandomPerspective(distortion_scale=0.1),\n",
    "                                  #v2.GaussianBlur(kernel_size=7, sigma=(0.1, 1.0)),\n",
    "                                  #v2.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2,hue=0)\n",
    "                                  # 0.2 means the brightness / contrast / saturation alternation range from [0.8~1.2] of original.\n",
    "                                  ])  # Image Augmented Transformation\n",
    "                                    # here is a shuffle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:02:35.924884300Z",
     "start_time": "2025-08-11T16:02:35.839999100Z"
    }
   },
   "id": "1cf66e59d1468e01",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 892 cancer cells and 167 normal cells, 1059 cells in total, for training and validating\n",
      "we have 158 cancer cells and 30 normal cells, 188 cells in total, for testing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "for i in dataset.samples:\n",
    "    image_paths.append(i[0])\n",
    "    image_labels.append(i[1])\n",
    "\n",
    "# split dataset into test set and (train set + validation set)\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
    "    image_paths, image_labels, test_size=test_percent, stratify=image_labels, random_state=seed)\n",
    "# here is a shuffle\n",
    "\n",
    "logger.info(\n",
    "    f'we have {train_val_labels.count(0)} cancer cells and {train_val_labels.count(1)} normal cells, {len(train_val_labels)} cells in total, for training and validating')\n",
    "logger.info(\n",
    "    f'we have {test_labels.count(0)} cancer cells and {test_labels.count(1)} normal cells, {len(test_paths)} cells in total, for testing')\n",
    "\n",
    "# save the test image to target folders\n",
    "for i in range(len(test_paths)):\n",
    "    for j in range(len(dataset.samples)):\n",
    "        if dataset.samples[j][0] == test_paths[i] and dataset.samples[j][1] == 0:\n",
    "            utils.save_image(dataset[j][0],\n",
    "                             test_autoseg_noblue_cancer_path + test_paths[i].split('\\\\')[-1].split('.')[0] + \"_test.png\")\n",
    "            break\n",
    "        elif dataset.samples[j][0] == test_paths[i] and dataset.samples[j][1] == 1:\n",
    "            utils.save_image(dataset[j][0],\n",
    "                             test_autoseg_noblue_normal_path + test_paths[i].split('\\\\')[-1].split(\".\")[0] + \"_test.png\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:02:49.383580500Z",
     "start_time": "2025-08-11T16:02:41.030782600Z"
    }
   },
   "id": "b16f167495a19c74",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.0167, 0.0171, 0.0000])\n",
      "Std: tensor([0.0388, 0.0276, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# train and validation dataset for calculating the image mean and std for normalization\n",
    "train_val_dataset = []\n",
    "for i in range(len(train_val_paths)):\n",
    "    for j in range(len(dataset.samples)):\n",
    "        if dataset.samples[j][0] == train_val_paths[i]:\n",
    "            train_val_dataset.append(dataset[j][0])\n",
    "            break\n",
    "\n",
    "images_mean, images_std = compute_mean_std(train_val_dataset, channels)\n",
    "logger.info(f\"Mean: {images_mean}\")\n",
    "logger.info(f\"Std: {images_std}\")\n",
    "\n",
    "# inverse_transform, to restore the images when saving them to whole folder and displaying them in XAI\n",
    "transform_inverse = v2.Compose([v2.Normalize(\n",
    "    mean=[-images_mean[0] / images_std[0], -images_mean[1] / images_std[1], -images_mean[2] / images_std[2]],\n",
    "    std=[1 / images_std[0], 1 / images_std[1], 1 / images_std[2]])])  # when mean = images_mean and std = images_std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:03:05.461136200Z",
     "start_time": "2025-08-11T16:02:51.337883800Z"
    }
   },
   "id": "b461dd33b9f6a95c",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save the validate image to target folders\n",
    "def save_validation_images(val_paths, dataset, cancer_path, normal_path):\n",
    "    for i in range(len(val_paths)):\n",
    "        for j in range(len(dataset.samples)):\n",
    "            if dataset.samples[j][0] == val_paths[i] and dataset.samples[j][1] == 0:\n",
    "                utils.save_image(dataset[j][0],\n",
    "                                 cancer_path + val_paths[i].split('\\\\')[-1].split('.')[0] + \"_val.png\")\n",
    "                break\n",
    "            elif dataset.samples[j][0] == val_paths[i] and dataset.samples[j][1] == 1:\n",
    "                utils.save_image(dataset[j][0],\n",
    "                                 normal_path + val_paths[i].split('\\\\')[-1].split(\".\")[0] + \"_val.png\")\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:03:27.599688400Z",
     "start_time": "2025-08-11T16:03:27.539093900Z"
    }
   },
   "id": "4ebf3cc4abeb2e86",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_save_train_images(train_paths, dataset, cancer_path, normal_path):\n",
    "    # original training images\n",
    "    train_origin_cancer_dataset = []\n",
    "    train_origin_normal_dataset = []\n",
    "    for i in range(len(train_paths)):\n",
    "        for j in range(len(dataset.samples)):\n",
    "            if dataset.samples[j][0] == train_paths[i] and dataset.samples[j][1] == 0:\n",
    "                train_origin_cancer_dataset.append(\n",
    "                    {'image': dataset[j][0], 'filename': train_paths[i].split('\\\\')[-1].split('.')[0]})\n",
    "                break\n",
    "            elif dataset.samples[j][0] == train_paths[i] and dataset.samples[j][1] == 1:\n",
    "                train_origin_normal_dataset.append(\n",
    "                    {'image': dataset[j][0], 'filename': train_paths[i].split('\\\\')[-1].split('.')[0]})\n",
    "                break\n",
    "    \n",
    "    # Initialize an empty list to store the augmented images\n",
    "    augmented_images_class_0 = []\n",
    "    augmented_images_class_1 = []\n",
    "    \n",
    "    # Image augmentation\n",
    "    for image in train_origin_cancer_dataset:\n",
    "        for i in range(num_variations_per_image_0):\n",
    "            augmented_images_class_0.append({'image': transform_augmented(image['image']), 'filename': image['filename']})\n",
    "    \n",
    "    for image in train_origin_normal_dataset:\n",
    "        for i in range(num_variations_per_image_1):\n",
    "            augmented_images_class_1.append({'image': transform_augmented(image['image']), 'filename': image['filename']})\n",
    "    \n",
    "    # save training dataset (original + augmentation)\n",
    "    if num_variations_per_image_0 > 0:\n",
    "        for i in range(len(augmented_images_class_0)):\n",
    "            utils.save_image(augmented_images_class_0[i]['image'],\n",
    "                             cancer_path + str(int(i / num_variations_per_image_0)) + \"_\" + str(\n",
    "                                 i % num_variations_per_image_0) + \"_\" + augmented_images_class_0[i][\n",
    "                                 'filename'] + \"_aug.png\")\n",
    "    \n",
    "    if num_variations_per_image_1 > 0:\n",
    "        for i in range(len(augmented_images_class_1)):\n",
    "            utils.save_image(augmented_images_class_1[i]['image'],\n",
    "                             normal_path + str(int(i / num_variations_per_image_1)) + \"_\" + str(\n",
    "                                 i % num_variations_per_image_1) + \"_\" + augmented_images_class_1[i][\n",
    "                                 'filename'] + \"_aug.png\")\n",
    "    \n",
    "    # save original images (after resize)\n",
    "    for i in range(len(train_origin_cancer_dataset)):\n",
    "        utils.save_image(train_origin_cancer_dataset[i]['image'],\n",
    "                         cancer_path + str(i) + \"_\" + train_origin_cancer_dataset[i][\n",
    "                             'filename'] + \"_original.png\")\n",
    "    \n",
    "    for i in range(len(train_origin_normal_dataset)):\n",
    "        utils.save_image(train_origin_normal_dataset[i]['image'],\n",
    "                         normal_path + str(i) + \"_\" + train_origin_normal_dataset[i][\n",
    "                             'filename'] + \"_original.png\")\n",
    "    \n",
    "    logger.info(\n",
    "        f\"we have generate {len(augmented_images_class_0)} augmented cancer cell images and {len(augmented_images_class_1)} augmented normal cell images.\")\n",
    "    logger.info(\n",
    "        f'Totally we have {len(train_origin_cancer_dataset) + len(augmented_images_class_0)} cancer cell images and {len(train_origin_normal_dataset) + len(augmented_images_class_1)} normal cell images for training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:03:28.301885Z",
     "start_time": "2025-08-11T16:03:28.258561600Z"
    }
   },
   "id": "bef2f53a6a4bd578",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_data(test_path, validation_path, train_path):\n",
    "    \n",
    "    # read test data\n",
    "    test_dataset = torchvision.datasets.ImageFolder(test_path, transform=transform_whole_dataset)\n",
    "    # read validation data\n",
    "    val_dataset = torchvision.datasets.ImageFolder(validation_path, transform=transform_whole_dataset)\n",
    "    # read train data\n",
    "    train_dataset = torchvision.datasets.ImageFolder(train_path, transform=transform_whole_dataset)\n",
    "    \n",
    "    return  test_dataset, val_dataset, train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:03:29.289044900Z",
     "start_time": "2025-08-11T16:03:29.247454700Z"
    }
   },
   "id": "f4639130759587ba",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_whole_image(val_dataset, test_dataset, validation_whole_path, test_whole_path): \n",
    "    # save test and validate image (un-shuffle, easy to find which one is misclassified)\n",
    "    for i in range(len(val_dataset)):\n",
    "        utils.save_image(transform_inverse(val_dataset[i][0]),\n",
    "                         validation_whole_path + str(i) + \"_\" + str(val_dataset[i][1]) + \"_\" +\n",
    "                         val_dataset.samples[i][0].split('\\\\')[-1].split('.')[0] + \".png\")\n",
    "    for i in range(len(test_dataset)):\n",
    "        utils.save_image(transform_inverse(test_dataset[i][0]),\n",
    "                         test_whole_path + str(i) + \"_\" + str(test_dataset[i][1]) + \"_\" +\n",
    "                         test_dataset.samples[i][0].split('\\\\')[-1].split('.')[0] + \".png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:03:37.185235700Z",
     "start_time": "2025-08-11T16:03:37.155175Z"
    }
   },
   "id": "4163d7e243b382b4",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation openness is opened\n",
      "In fold 0\n",
      "we have 713 cancer cells and 134 normal cells, 847 cells in total, for training\n",
      "we have 179 cancer cells and 33 normal cells, 212 cells in total, for validating\n",
      "we have generate 713 augmented cancer cell images and 1072 augmented normal cell images.\n",
      "Totally we have 1426 cancer cell images and 1206 normal cell images for training\n",
      "In fold 1\n",
      "we have 713 cancer cells and 134 normal cells, 847 cells in total, for training\n",
      "we have 179 cancer cells and 33 normal cells, 212 cells in total, for validating\n",
      "we have generate 713 augmented cancer cell images and 1072 augmented normal cell images.\n",
      "Totally we have 1426 cancer cell images and 1206 normal cell images for training\n",
      "In fold 2\n",
      "we have 714 cancer cells and 133 normal cells, 847 cells in total, for training\n",
      "we have 178 cancer cells and 34 normal cells, 212 cells in total, for validating\n",
      "we have generate 714 augmented cancer cell images and 1064 augmented normal cell images.\n",
      "Totally we have 1428 cancer cell images and 1197 normal cell images for training\n",
      "In fold 3\n",
      "we have 714 cancer cells and 133 normal cells, 847 cells in total, for training\n",
      "we have 178 cancer cells and 34 normal cells, 212 cells in total, for validating\n",
      "we have generate 714 augmented cancer cell images and 1064 augmented normal cell images.\n",
      "Totally we have 1428 cancer cell images and 1197 normal cell images for training\n",
      "In fold 4\n",
      "we have 714 cancer cells and 134 normal cells, 848 cells in total, for training\n",
      "we have 178 cancer cells and 33 normal cells, 211 cells in total, for validating\n",
      "we have generate 714 augmented cancer cell images and 1072 augmented normal cell images.\n",
      "Totally we have 1428 cancer cell images and 1206 normal cell images for training\n"
     ]
    }
   ],
   "source": [
    "# This transform: to tensor and normalization is used for all images\n",
    "transform_whole_dataset = v2.Compose([v2.ToTensor(), v2.Normalize(mean=images_mean, std=images_std)])\n",
    "\n",
    "if not cross_validation:\n",
    "    # split train_val_dataset into train set and validation set\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_val_paths, train_val_labels, test_size=validation_percent / (1 - test_percent), stratify=train_val_labels,\n",
    "        random_state=seed) # here is a shuffle\n",
    "    \n",
    "    logger.info(f'The cross validation openness is closed')\n",
    "    logger.info(\n",
    "        f'we have {train_labels.count(0)} cancer cells and {train_labels.count(1)} normal cells, {len(train_paths)} cells in total, for training')\n",
    "    logger.info(\n",
    "        f'we have {val_labels.count(0)} cancer cells and {val_labels.count(1)} normal cells, {len(val_paths)} cells in total, for validating')\n",
    "    \n",
    "    save_validation_images(val_paths, dataset, validation_autoseg_noblue_cancer_path, validation_autoseg_noblue_normal_path)\n",
    "    generate_save_train_images(train_paths, dataset, train_autoseg_noblue_cancer_path, train_autoseg_noblue_normal_path)\n",
    "    \n",
    "    # read test, train, validate data from target folders\n",
    "    test_dataset = []\n",
    "    val_dataset = []\n",
    "    train_dataset = []\n",
    "    test, val, train = read_data(test_autoseg_noblue_path, validation_autoseg_noblue_path, train_autoseg_noblue_path)\n",
    "    test_dataset.append(test)\n",
    "    val_dataset.append(val)\n",
    "    train_dataset.append(train)\n",
    "    \n",
    "    save_whole_image(val, test, validation_whole_autoseg_noblue_path, test_whole_autoseg_noblue_path)\n",
    "else:\n",
    "    logger.info(f'The cross validation openness is opened')\n",
    "    test_dataset = []\n",
    "    val_dataset = []\n",
    "    train_dataset = []\n",
    "    skf = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed) # here is a shuffle\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_val_paths, train_val_labels)):\n",
    "        # use Stratified K-fold cross validation to create e.g. 10 folds, in each fold, the ratio between normal and cancer approximately keeping the same as the ratio in original image dataset.\n",
    "        train_paths = [train_val_paths[i] for i in train_idx]\n",
    "        train_labels = [train_val_labels[i] for i in train_idx]\n",
    "        \n",
    "        val_paths = [train_val_paths[i] for i in val_idx]\n",
    "        val_labels = [train_val_labels[i] for i in val_idx]\n",
    "        \n",
    "        logger.info(f\"In fold {fold}\")\n",
    "        logger.info(\n",
    "        f'we have {train_labels.count(0)} cancer cells and {train_labels.count(1)} normal cells, {len(train_paths)} cells in total, for training')\n",
    "        logger.info(\n",
    "        f'we have {val_labels.count(0)} cancer cells and {val_labels.count(1)} normal cells, {len(val_paths)} cells in total, for validating')\n",
    "        \n",
    "        # new folders to save validation images for each fold.\n",
    "        validation_cv_fold_path = validation_autoseg_noblue_cv_path + \"cross validation \" + str(fold)\n",
    "        validation_cv_fold_split_path = validation_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\split'\n",
    "        validation_cv_fold_whole_path = validation_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\whole'\n",
    "        validation_cv_fold_split_cancer_path = validation_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\cancer'\n",
    "        validation_cv_fold_split_normal_path = validation_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\normal'\n",
    "        os.makedirs(validation_cv_fold_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_split_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_whole_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_split_cancer_path, exist_ok=True)\n",
    "        os.makedirs(validation_cv_fold_split_normal_path, exist_ok=True)\n",
    "        \n",
    "        save_validation_images(val_paths, dataset, validation_cv_fold_split_cancer_path + '\\\\', validation_cv_fold_split_normal_path + '\\\\')\n",
    "        \n",
    "        # new folders to save train images for each fold.\n",
    "        train_cv_fold_path = train_autoseg_noblue_cv_path + \"cross validation \" + str(fold)\n",
    "        train_cv_fold_split_path = train_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\split'\n",
    "        train_cv_fold_whole_path = train_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\whole'\n",
    "        train_cv_fold_split_cancer_path = train_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\cancer'\n",
    "        train_cv_fold_split_normal_path = train_autoseg_noblue_cv_path + \"cross validation \" + str(fold) + '\\\\split\\\\normal'\n",
    "        os.makedirs(train_cv_fold_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_split_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_whole_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_split_cancer_path, exist_ok=True)\n",
    "        os.makedirs(train_cv_fold_split_normal_path, exist_ok=True)\n",
    "        \n",
    "        generate_save_train_images(train_paths, dataset, train_cv_fold_split_cancer_path + '\\\\', train_cv_fold_split_normal_path + '\\\\')\n",
    "        \n",
    "        # read test, train, validate data from target folders\n",
    "        test, val, train = read_data(test_autoseg_noblue_path, validation_cv_fold_split_path, train_cv_fold_split_path)\n",
    "        test_dataset.append(test)\n",
    "        val_dataset.append(val)\n",
    "        train_dataset.append(train)\n",
    "        \n",
    "        save_whole_image(val, test, validation_cv_fold_whole_path + '\\\\', test_whole_autoseg_noblue_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:29:13.944790900Z",
     "start_time": "2025-08-11T16:19:00.843292800Z"
    }
   },
   "id": "95ec40b82d31231c",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "567b1d33afd8e429"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import nn, optim\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "# hyper-parameters set for training the model\n",
    "# users can make the length of each list = 1 to close grid search\n",
    "param_grid = {\n",
    "'lr_backbone': [0.00001, 0.00005],\n",
    "'lr_head': [0.0005, 0.0002],\n",
    "'lr_channel': [0.0001, 0.0003],\n",
    "'weight_decay_backbone': [0.001],\n",
    "'weight_decay_head': [0.0],\n",
    "'weight_decay_channel': [0.0],\n",
    "'dropout_p': [0.0, 0.3, 0.5],\n",
    "'warmup_epoch': [5, 10],\n",
    "'lr_decay_epoch': [30, 40],\n",
    "'unfrozen_blocks': [[0,1,2,9,10,11], [0,1,2,3,4,5,6,7,8,9,10,11]],\n",
    "'grad_clip':[False, True]\n",
    "}\n",
    "'''\n",
    "param_grid = {\n",
    "'lr_backbone': [0.00002],\n",
    "'lr_head': [0.0002],\n",
    "'lr_channel': [0.0001],\n",
    "'weight_decay_backbone': [0.001],\n",
    "'weight_decay_head': [0.000],\n",
    "'weight_decay_channel': [0.000],\n",
    "'dropout_p': [0.3],\n",
    "'warmup_epoch': [5],\n",
    "'lr_decay_epoch': [30],\n",
    "'unfrozen_blocks': [[0,1,2,3,4,5,6,7,8,9,10,11]],\n",
    "'grad_clip': [False]\n",
    "}    \n",
    "\n",
    "grid = list(ParameterGrid(param_grid)) # generate all the hyper-parameter combination\n",
    "gird_search_result = [] # store the cross-validation performance of each hyper-parameter set\n",
    "for set_num, hyper_params in enumerate(grid):\n",
    "    logger.info(f\"Running config {set_num + 1}/{len(grid)}: {hyper_params}\")\n",
    "    \n",
    "    config = {\n",
    "    'lr_backbone': hyper_params['lr_backbone'],\n",
    "    'lr_head': hyper_params['lr_head'],\n",
    "    'lr_channel': hyper_params['lr_channel'],\n",
    "    'weight_decay_backbone': hyper_params['weight_decay_backbone'],\n",
    "    'weight_decay_head': hyper_params['weight_decay_head'],\n",
    "    'weight_decay_channel': hyper_params['weight_decay_channel'],\n",
    "    'dropout_p': hyper_params['dropout_p'],\n",
    "    'num_epochs': 60,\n",
    "    'warmup_epoch': hyper_params['warmup_epoch'],\n",
    "    'lr_decay_epoch': hyper_params['lr_decay_epoch'],\n",
    "    'unfrozen_blocks': hyper_params['unfrozen_blocks'],\n",
    "    'grad_clip': hyper_params['grad_clip']\n",
    "    } # configure all the hyper-parameters, including not for grid search\n",
    "\n",
    "    for fold in range(fold_num):\n",
    "        logger.info('###############################################')\n",
    "        logger.info(f'This is the fold: {fold}')\n",
    "        logger.info('###############################################')\n",
    "        # Load the train, validate, and test dataset\n",
    "        #torch.manual_seed(seed) # if the shuffle is displayed different each time, use torch.manual_seed(seed)\n",
    "        train_loader = DataLoader(train_dataset[fold], batch_size=batch_size, shuffle=True)  # here is a shuffle\n",
    "        val_loader = DataLoader(val_dataset[fold], batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset[fold], batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            logger.info(f\"Train batch images shape: {images.shape}\")\n",
    "            logger.info(f\"Train batch labels: {labels}\")\n",
    "            break\n",
    "        for images, labels in val_loader:\n",
    "            logger.info(f\"Validate batch images shape: {images.shape}\")\n",
    "            logger.info(f\"Validate batch labels: {labels}\")\n",
    "            break\n",
    "        for images, labels in test_loader:\n",
    "            logger.info(f\"Test batch images shape: {images.shape}\")\n",
    "            logger.info(f\"Test batch labels: {labels}\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        # Set gpu/cpu\n",
    "        logger.info(f\"Use device: {device}\")\n",
    "        \n",
    "        # Model Params\n",
    "        PATCH_SIZE = 16\n",
    "        EMBED_DIM = 192\n",
    "        RETURN_ALL_TOKENS = False\n",
    "        MAX_NUMBER_CHANNELS = 3\n",
    "        \n",
    "        # use chadavit model\n",
    "        model = ChAdaViT(\n",
    "            patch_size=PATCH_SIZE,\n",
    "            embed_dim=EMBED_DIM,\n",
    "            return_all_tokens=RETURN_ALL_TOKENS,\n",
    "            max_number_channels=MAX_NUMBER_CHANNELS,\n",
    "        )\n",
    "        \n",
    "        assert (\n",
    "            CKPT_PATH.endswith(\".ckpt\")\n",
    "            or CKPT_PATH.endswith(\".pth\")\n",
    "            or CKPT_PATH.endswith(\".pt\")\n",
    "        ) # ensure the CKPT_PATH ends correctly\n",
    "        state = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)[\"state_dict\"]\n",
    "        for k in list(state.keys()):\n",
    "            if \"encoder\" in k:\n",
    "                state[k.replace(\"encoder\", \"backbone\")] = state[k]\n",
    "            if \"backbone\" in k:\n",
    "                state[k.replace(\"backbone.\", \"\")] = state[k]\n",
    "            del state[k]\n",
    "        for k in list(state.keys()):\n",
    "            if \"channel_token\" == k:\n",
    "                w = state[k]\n",
    "                state[k] = w[:, :MAX_NUMBER_CHANNELS, :, :].clone()\n",
    "        # select first three vectors in original channel_token\n",
    "        model.load_state_dict(state, strict=False) # load the pre-trained parameter\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        class ModifiedChadaViT(nn.Module):\n",
    "            def __init__(self, base_model):\n",
    "                super(ModifiedChadaViT, self).__init__()\n",
    "                self.base_model = base_model\n",
    "                self.head = nn.Sequential(nn.Linear(base_model.embed_dim, 768),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Dropout(config['dropout_p']),\n",
    "                                          nn.Linear(768, 2))  # modify the head from Identify to 2-class classification (Linear Layer) and add drop out (included in grid search)\n",
    "        \n",
    "            def forward(self, x, list_num_channels, index=0):\n",
    "                # the features from ViT backbone (batch size, 768 (dim of class token))\n",
    "                features = self.base_model(x=x, index=index, list_num_channels=[list_num_channels])\n",
    "                # pass the classification head\n",
    "                return self.head(features)\n",
    "        \n",
    "        \n",
    "        model = ModifiedChadaViT(model).to(device)  # TODO: use the basic ViT-B-16 model, see the most below\n",
    "        model.mixed_channels = mixed_channels # all of the inputs share the same channel number.\n",
    "        \n",
    "        for param in model.base_model.blocks.parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer_index in config['unfrozen_blocks']:\n",
    "            layer = model.base_model.blocks[layer_index]  \n",
    "            # only un-froze the last few layers (included in grid search)\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        for param in model.head.parameters(): # un froze the classification head\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        class EarlyStopping:\n",
    "            def __init__(self, patience=5, mode='loss', min_delta=0.0, fold=0, epoch_number = 10):\n",
    "                # min_delta used to measure the extension of loss decrease, only the loss decrease > min_delta, we can say early stop check pass.\n",
    "                self.patience = patience\n",
    "                self.mode = mode\n",
    "                self.fold = fold\n",
    "                self.min_delta = min_delta\n",
    "                self.counter = 0\n",
    "                self.epoch_number = epoch_number\n",
    "                self.best = None\n",
    "                self.early_stop = False\n",
    "        \n",
    "            def __call__(self, val_loss, accuracy, epoch):\n",
    "                logger.info(\"the mode of early stopping is \" + self.mode)\n",
    "                if self.mode == 'loss':\n",
    "                    if self.best is None:\n",
    "                        logger.info(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = val_loss\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best val_loss is: {self.best}\")\n",
    "                    elif val_loss < self.best - self.min_delta:\n",
    "                        logger.info(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = val_loss\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best val_loss is: {self.best}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        logger.info(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        logger.info(f\"The best val_loss is still: {self.best}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "                elif self.mode == 'accuracy':\n",
    "                    if self.best is None:\n",
    "                        logger.info(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best accuracy / weighted f1 score is: {self.best[1]}\")\n",
    "                    elif accuracy > self.best[1]:\n",
    "                        logger.info(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best accuracy / weighted f1 score is: {self.best[1]}\")\n",
    "                    elif accuracy == self.best[1] and val_loss < self.best[0]:\n",
    "                        logger.info(f\"Epoch {epoch + 1} Early Stop Check Pass! val_loss is smaller although the accuracy / weighted f1 score keeps the same.\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best accuracy / weighted f1 score is: {self.best[1]}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        logger.info(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        logger.info(f\"The best accuracy / weighted f1 score is still: {self.best[1]}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "                elif self.mode == 'loss and accuracy':\n",
    "                    if self.best is None:\n",
    "                        logger.info(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    elif val_loss < self.best[0] - self.min_delta and accuracy > self.best[1]:\n",
    "                        logger.info(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        logger.info(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        logger.info(f\"The best val_loss and accuracy / weighted f1 score are still: {self.best[0]}, {self.best[1]}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "                elif self.mode == 'loss or accuracy':\n",
    "                    if self.best is None:\n",
    "                        logger.info(f\"This is the first epoch {epoch + 1}!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    elif val_loss < self.best[0] - self.min_delta or accuracy > self.best[1]:\n",
    "                        logger.info(f\"Epoch {epoch + 1} Early Stop Check Pass!\")\n",
    "                        self.best = [val_loss, accuracy]\n",
    "                        self.counter = 0\n",
    "                        torch.save(model.state_dict(), \"best_model_parameter_\" + str(self.fold) + \".pth\")\n",
    "                        logger.info(f\"The best val_loss and accuracy / weighted f1 score are: {self.best[0]}, {self.best[1]}\")\n",
    "                    else:\n",
    "                        self.counter += 1\n",
    "                        logger.info(\n",
    "                            f\"Epoch {epoch + 1} not pass the Early Stopping! EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "                        logger.info(f\"The best val_loss and accuracy / weighted f1 score are still: {self.best[0]}, {self.best[1]}\")\n",
    "                        if self.counter >= self.patience or (epoch + 1) == self.epoch_number:\n",
    "                            self.early_stop = True\n",
    "        \n",
    "        num_epochs = config['num_epochs']\n",
    "        warm_up_epochs = config['warmup_epoch']\n",
    "        total_steps = (config['warmup_epoch'] + config['lr_decay_epoch']) * len(train_loader)\n",
    "        warm_up_steps = warm_up_epochs * len(train_loader)\n",
    "        # Set L.F., optimizer\n",
    "        #weights = torch.tensor([1.0, 1.5]).to(device) # set higher weights for positive class in CEL\n",
    "        #criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        channel_params = [p for n, p in model.base_model.named_parameters() if n == \"channel_token\"]\n",
    "        backbone_params = [\n",
    "            p for n, p in model.base_model.named_parameters()\n",
    "            if n != \"channel_token\" and p.requires_grad\n",
    "        ]\n",
    "        head_params = list(model.head.parameters())\n",
    "        optimizer = optim.AdamW([{'params': backbone_params, 'weight_decay': config['weight_decay_backbone'], 'lr': config['lr_backbone']}, {'params': head_params, 'weight_decay': config['weight_decay_head'], 'lr': config['lr_head']}, {'params': channel_params, 'weight_decay': config['weight_decay_channel'], 'lr': config['lr_channel']}], betas=(0.9, 0.98)) # modify the bedats from (0.9, 0.999) to (0.9, 0.98) to prevent the gradients from being influenced by previous gradients a lot.\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        # weight_decay is not always good, see notebook 55 for detail\n",
    "        scheduler = get_cosine_with_warmup_tail(optimizer, num_warmup_steps=warm_up_steps, num_training_steps=total_steps, min_lr_factor=0.1)  # the learning rate will warm-up firstly, then cosine decay to 0.1 of the initial lr.\n",
    "        \n",
    "        \n",
    "        # TODO: the normalization need to be mean = [0.485, 0.456, 0.406] std  = [0.229, 0.224, 0.225]. ------ not good, use the image_mean and image_std will let the model converge better.\n",
    "        # TODO: Set an Early stop and try epoch = 10 or more. ------ Done\n",
    "        # TODO: Try another kind of lr-scheduler, lr value or let the gamma smaller, maybe 0.25\n",
    "        # TODO: In optimizer, use momentum? ------ Adam and AdamW don't need momentum, they are carried with momentum. Only SGD needs momentum, but prefer to using AdamW in ViT training\n",
    "        # TODO: Use another optimizer? AdamW, SGD, etc. ------ Done\n",
    "        # TODO: Use weight decay? dropout in the classifier ? ------ Done\n",
    "        # TODO: Use warm-up + learning rate scheduler combination? ------ Done\n",
    "        # TODO: Change the size of Batch?\n",
    "        # TODO: Use hyperparameter search algorithm, like random search, grid search, etc. ------ Done\n",
    "        # TODO: Use TensorBoard to monitor the training process\n",
    "        # TODO: Use other kinds of augmented methods\n",
    "        # TODO: add label smoothing for small dataset and focal loss for hard to classify sample\n",
    "        # TODO: Freeze fewer blocks, and un-freeze position embedding, layernorm ------ Done\n",
    "        # TODO: Try hybrid ViT, Swin-ViT (both have CNN’s properties) or Dei-T (better on small dataset)\n",
    "        # TODO: 再看几篇paper的experiment，看看别人是怎么做的\n",
    "        # TODO: ViT should be put into comparison model set\n",
    "        \n",
    "        # train func\n",
    "        def train(model, loader, criterion, optimizer, device, epoch, train_dataset):\n",
    "            model.train()  # set model to train mode\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(loader, 0):  # each batch input\n",
    "                inputs, targets = data\n",
    "                targets = targets.to(device)\n",
    "                inputs = collate_images(inputs) # collate the input image batch to a sequence of channel\n",
    "                X, list_num_channels = inputs\n",
    "                X = X.to(device)\n",
    "                optimizer.zero_grad()  # reset the grads\n",
    "                outputs = model(X, list_num_channels, index=0)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()  # back propagation\n",
    "                if config['grad_clip']:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # gradients clip\n",
    "                optimizer.step()  # update parameter\n",
    "                running_loss += loss.item() * targets.size(0)\n",
    "                logger.info(\"In epoch \" + str(epoch + 1) + \", batch: \" + str(i + 1) + \", average loss per image: \" + str(\n",
    "                    loss.item()))\n",
    "        \n",
    "                correct_train = 0\n",
    "                _, predicted_train = outputs.max(1)  # model predicted class\n",
    "                correct_train += (predicted_train == targets).sum().item()\n",
    "                logger.info(\"Accuracy of the network on the train set: \" + str(correct_train / targets.size(0)))\n",
    "                \n",
    "                scheduler.step()  # regularize the learning rate\n",
    "        \n",
    "            return running_loss / len(train_dataset)\n",
    "        \n",
    "        \n",
    "        # validate func\n",
    "        def validate(model, loader, device, val_dataset):\n",
    "            model.eval()  # set model to validate mode\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            false = []\n",
    "            running_loss = 0.0\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(loader, 0):\n",
    "                    inputs, targets = data\n",
    "                    targets = targets.to(device)\n",
    "                    inputs = collate_images(inputs) # collate the input image batch to a sequence of channel\n",
    "                    X, list_num_channels = inputs\n",
    "                    X = X.to(device)\n",
    "                    outputs = model(X, list_num_channels, index=0)\n",
    "                    _, predicted = outputs.max(1)  # model predicted class\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "                    total += targets.size(0)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    running_loss += loss.item() * targets.size(0) # the sum of val_loss in one batch            \n",
    "                    \n",
    "                    for target in targets:\n",
    "                        y_true.append(target.item())\n",
    "                    for predict in predicted:\n",
    "                        y_pred.append(predict.item())\n",
    "                    \n",
    "                    for result in range(len(predicted)): # collect wrong samples\n",
    "                        if predicted[result] != targets[result]:\n",
    "                            false.append(i * batch_size + result)\n",
    "                    \n",
    "                y_true = np.array(y_true)\n",
    "                y_pred = np.array(y_pred)\n",
    "                \n",
    "            return correct / total, false, running_loss / len(val_dataset), f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        \n",
    "        # train and validate\n",
    "        early_stopping = EarlyStopping(patience=10, mode=early_stop_mode, fold=fold, epoch_number = num_epochs)\n",
    "        wrong_number = []\n",
    "        \n",
    "        training_loss_list = []\n",
    "        validating_loss_list = []\n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f\"The learning rate of backbone is: {optimizer.param_groups[0]['lr']}, of head is {optimizer.param_groups[1]['lr']}, of channel token is {optimizer.param_groups[2]['lr']}\")\n",
    "            train_loss = train(model, train_loader, criterion, optimizer, device, epoch, train_dataset[fold])\n",
    "            accuracy, wrong_predicted, val_loss, weighted_f1 = validate(model, val_loader, device, val_dataset[fold])\n",
    "            wrong_number.append(wrong_predicted)\n",
    "            logger.info(\"====================\" + str(epoch + 1) + \"====================\")\n",
    "            logger.info(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            logger.info(f'Average train loss per image: {train_loss:.7f}')\n",
    "            logger.info(f'Average validate loss per image: {val_loss:.7f}')\n",
    "            logger.info(f'Validate accuracy: {accuracy:.4f}')\n",
    "            logger.info(\"====================\" + str(epoch + 1) + \"====================\")\n",
    "            \n",
    "            training_loss_list.append(train_loss)\n",
    "            validating_loss_list.append(val_loss)\n",
    "            early_stopping(val_loss, weighted_f1, epoch)\n",
    "        \n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\" 🔥 Early stopping, Stop Training\")\n",
    "                logger.info(f\"select the epoch: {epoch - early_stopping.counter + 1}\")\n",
    "                for wrong_result in wrong_number[epoch - early_stopping.counter]:\n",
    "                    logger.info(\"The number \" + str(wrong_result) + \" is wrong!\")\n",
    "                break\n",
    "    \n",
    "            if epoch == num_epochs - 1:\n",
    "                logger.info(\"train until the last epoch!\")\n",
    "                for wrong_result in wrong_predicted:\n",
    "                    logger.info(\"The number \" + str(wrong_result) + \" is wrong!\")\n",
    "                    \n",
    "        plot_loss_curves(training_loss_list, validating_loss_list, fold_number = fold, hyper_setnum=set_num, model_type=\"Channel wise DINO\")\n",
    "                    \n",
    "    model_accuracy = []\n",
    "    model_recall = []\n",
    "    model_precision = []\n",
    "    model_auc_roc_macro = []\n",
    "    model_auc_roc_micro = []\n",
    "    model_auc_roc_weighted = []\n",
    "    model_f1_macro = []\n",
    "    model_f1_micro = []\n",
    "    model_f1_weighted = []\n",
    "    \n",
    "    \n",
    "    for fold in range(fold_num):\n",
    "        model.load_state_dict(torch.load(\"best_model_parameter_\" + str(fold) + \".pth\"))\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_prob = []\n",
    "        \n",
    "        for data in range(len(val_dataset[fold])):\n",
    "            y_true.append(val_dataset[fold][data][1])\n",
    "        \n",
    "            inputs = val_dataset[fold][data][0]\n",
    "            inputs = inputs.to(device)\n",
    "            input_tensor = collate_images(inputs.unsqueeze(0))\n",
    "            X, list_num_channels = input_tensor\n",
    "            X = X.to(device)\n",
    "            outputs = model(X, list_num_channels, index=0)\n",
    "            predicted_class = outputs.argmax(dim=1).item()\n",
    "            y_pred.append(predicted_class)\n",
    "        \n",
    "            prob = torch.softmax(outputs, dim=1)[:, 1].item()\n",
    "            y_prob.append(prob)\n",
    "        \n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_prob = np.array(y_prob)\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        auc_roc_macro = roc_auc_score(y_true, y_prob)\n",
    "        auc_roc_micro = roc_auc_score(y_true, y_prob)\n",
    "        auc_roc_weighted = roc_auc_score(y_true, y_prob)\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        accuracy = 0.0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == y_true[i]:\n",
    "                accuracy += 1.0\n",
    "        model_accuracy.append(accuracy/len(y_pred))\n",
    "        model_recall.append(recall)\n",
    "        model_precision.append(precision)\n",
    "        model_auc_roc_macro.append(auc_roc_macro)\n",
    "        model_auc_roc_micro.append(auc_roc_micro)\n",
    "        model_auc_roc_weighted.append(auc_roc_weighted)\n",
    "        model_f1_macro.append(f1_macro)\n",
    "        model_f1_micro.append(f1_micro)\n",
    "        model_f1_weighted.append(f1_weighted)\n",
    "        \n",
    "        logger.info(f\"Accuracy: {accuracy/len(y_pred):.4f}\")\n",
    "        logger.info(f\"Recall: {recall:.4f}\")\n",
    "        logger.info(f\"Precision: {precision:.4f}\")\n",
    "        logger.info(f\"AUC-ROC Macro: {auc_roc_macro:.4f}\")\n",
    "        logger.info(f\"AUC-ROC Micro: {auc_roc_micro:.4f}\")\n",
    "        logger.info(f\"AUC-ROC Weighted: {auc_roc_weighted:.4f}\")\n",
    "        logger.info(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "        logger.info(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "        logger.info(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "        \n",
    "    logger.info(f\"The mean and std of accuracy are: {np.array(model_accuracy).mean()}, and {np.array(model_accuracy).std()}\")\n",
    "    logger.info(f\"The mean and std of recall are: {np.array(model_recall).mean()}, and {np.array(model_recall).std()}\")\n",
    "    logger.info(f\"The mean and std of precision are: {np.array(model_precision).mean()}, and {np.array(model_precision).std()}\")\n",
    "    logger.info(f\"The mean and std of auc_roc_macro are: {np.array(model_auc_roc_macro).mean()}, and {np.array(model_auc_roc_macro).std()}\")\n",
    "    logger.info(f\"The mean and std of auc_roc_micro are: {np.array(model_auc_roc_micro).mean()}, and {np.array(model_auc_roc_micro).std()}\")\n",
    "    logger.info(f\"The mean and std of auc_roc_weighted are: {np.array(model_auc_roc_weighted).mean()}, and {np.array(model_auc_roc_weighted).std()}\")\n",
    "    logger.info(f\"The mean and std of f1_macro are: {np.array(model_f1_macro).mean()}, and {np.array(model_f1_macro).std()}\")\n",
    "    logger.info(f\"The mean and std of f1_micro are: {np.array(model_f1_micro).mean()}, and {np.array(model_f1_micro).std()}\")\n",
    "    logger.info(f\"The mean and std of f1_weighted are: {np.array(model_f1_weighted).mean()}, and {np.array(model_f1_weighted).std()}\")\n",
    "    \n",
    "    gird_search_result.append({'hyperparams': hyper_params,\n",
    "                               'accuracy mean':np.array(model_accuracy).mean(),\n",
    "                               'accuracy std': np.array(model_accuracy).std(),\n",
    "                               'recall mean': np.array(model_recall).mean(),\n",
    "                               'recall std': np.array(model_recall).std(),\n",
    "                               'precision mean': np.array(model_precision).mean(),\n",
    "                               'precision std': np.array(model_precision).std(),\n",
    "                               'auc_roc_macro mean': np.array(model_auc_roc_macro).mean(),\n",
    "                               'auc_roc_macro std': np.array(model_auc_roc_macro).std(),\n",
    "                               'auc_roc_micro mean': np.array(model_auc_roc_micro).mean(),\n",
    "                               'auc_roc_micro std': np.array(model_auc_roc_micro).std(),\n",
    "                               'auc_roc_weighted mean': np.array(model_auc_roc_weighted).mean(),\n",
    "                               'auc_roc_weighted std': np.array(model_auc_roc_weighted).std(),\n",
    "                               'f1_macro mean': np.array(model_f1_macro).mean(),\n",
    "                               'f1_macro std': np.array(model_f1_macro).std(),\n",
    "                               'f1_micro mean': np.array(model_f1_micro).mean(),\n",
    "                               'f1_micro std': np.array(model_f1_micro).std(),\n",
    "                               'f1_weighted mean': np.array(model_f1_weighted).mean(),\n",
    "                               'f1_weighted std': np.array(model_f1_weighted).std()})\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e15796e08c9ed584"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Here is the model evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "926e59c580921a9d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def inspect_model_and_optimizer(model, optimizer):\n",
    "    logger.info(\"===== Trainable Parameters in Model =====\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            logger.info(f\"[TRAINABLE] {name} | shape: {tuple(param.shape)} | numel: {param.numel():,}\")\n",
    "            total_params += param.numel()\n",
    "    logger.info(f\"Total trainable parameters in model: {total_params:,}\")\n",
    "    logger.info(\"=========================================\\n\")\n",
    "\n",
    "    logger.info(\"===== Optimizer Parameter Groups =====\")\n",
    "    for i, group in enumerate(optimizer.param_groups):\n",
    "        lr = group.get(\"lr\", None)\n",
    "        wd = group.get(\"weight_decay\", None)\n",
    "        logger.info(f\"-- Group {i}: lr={lr}, weight_decay={wd}\")\n",
    "        group_param_names = []\n",
    "        for p in group[\"params\"]:\n",
    "            for name, param in model.named_parameters():\n",
    "                if p is param:\n",
    "                    group_param_names.append(name)\n",
    "        for name in group_param_names:\n",
    "            logger.info(f\"    {name}\")\n",
    "        logger.info(f\"  -> Total params in group {i}: {sum(p.numel() for p in group['params']):,}\")\n",
    "    logger.info(\"=======================================\")\n",
    "inspect_model_and_optimizer(model, optimizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec9f5753cdd258db"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logger.info(model)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2067fe0eda842ac1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# print the validation performance of each hyper-parameter set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "452f002f33577ba2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for result in gird_search_result:\n",
    "    for key, value in result.items():\n",
    "        logger.info(f\"{key}: {value}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee0fe465513e55b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Here I use each model in cross validation (1 model if not using cross validation) to predict the test dataset, and calculate the mean and std of the performance metrics like accuracy, recall, precision, etc. If the mean is similar to the performance on validation dataset, I can say the generalizability of the trained model is satisfied, but if the robustness is large, I can say although the generalizability is good, it is not very robustness."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7fa364922c4dd7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "model.eval()\n",
    "\n",
    "\n",
    "model_accuracy = []\n",
    "model_recall = []\n",
    "model_precision = []\n",
    "model_auc_roc_macro = []\n",
    "model_auc_roc_micro = []\n",
    "model_auc_roc_weighted = []\n",
    "model_f1_macro = []\n",
    "model_f1_micro = []\n",
    "model_f1_weighted = []\n",
    "\n",
    "\n",
    "for fold in range(fold_num):\n",
    "    model.load_state_dict(torch.load(\"best_model_parameter_\" + str(fold) + \".pth\"))\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    \n",
    "    for data in range(len(test_dataset[fold])):\n",
    "        y_true.append(test_dataset[fold][data][1])\n",
    "    \n",
    "        inputs = test_dataset[fold][data][0]\n",
    "        inputs = inputs.to(device)\n",
    "        input_tensor = collate_images(inputs.unsqueeze(0))\n",
    "        X, list_num_channels = input_tensor\n",
    "        X = X.to(device)\n",
    "        outputs = model(X, list_num_channels, index=0)\n",
    "        predicted_class = outputs.argmax(dim=1).item()\n",
    "        y_pred.append(predicted_class)\n",
    "    \n",
    "        prob = torch.softmax(outputs, dim=1)[:, 1].item()\n",
    "        y_prob.append(prob)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    auc_roc_macro = roc_auc_score(y_true, y_prob)\n",
    "    auc_roc_micro = roc_auc_score(y_true, y_prob)\n",
    "    auc_roc_weighted = roc_auc_score(y_true, y_prob)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            accuracy += 1.0\n",
    "    model_accuracy.append(accuracy/len(y_pred))\n",
    "    model_recall.append(recall)\n",
    "    model_precision.append(precision)\n",
    "    model_auc_roc_macro.append(auc_roc_macro)\n",
    "    model_auc_roc_micro.append(auc_roc_micro)\n",
    "    model_auc_roc_weighted.append(auc_roc_weighted)\n",
    "    model_f1_macro.append(f1_macro)\n",
    "    model_f1_micro.append(f1_micro)\n",
    "    model_f1_weighted.append(f1_weighted)\n",
    "    \n",
    "    logger.info(f\"Accuracy: {accuracy/len(y_pred):.4f}\")\n",
    "    logger.info(f\"Recall: {recall:.4f}\")\n",
    "    logger.info(f\"Precision: {precision:.4f}\")\n",
    "    logger.info(f\"AUC-ROC Macro: {auc_roc_macro:.4f}\")\n",
    "    logger.info(f\"AUC-ROC Micro: {auc_roc_micro:.4f}\")\n",
    "    logger.info(f\"AUC-ROC Weighted: {auc_roc_weighted:.4f}\")\n",
    "    logger.info(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "    logger.info(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "    logger.info(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "    \n",
    "logger.info(f\"The mean and std of accuracy are: {np.array(model_accuracy).mean()}, and {np.array(model_accuracy).std()}\")\n",
    "logger.info(f\"The mean and std of recall are: {np.array(model_recall).mean()}, and {np.array(model_recall).std()}\")\n",
    "logger.info(f\"The mean and std of precision are: {np.array(model_precision).mean()}, and {np.array(model_precision).std()}\")\n",
    "logger.info(f\"The mean and std of auc_roc_macro are: {np.array(model_auc_roc_macro).mean()}, and {np.array(model_auc_roc_macro).std()}\")\n",
    "logger.info(f\"The mean and std of auc_roc_micro are: {np.array(model_auc_roc_micro).mean()}, and {np.array(model_auc_roc_micro).std()}\")\n",
    "logger.info(f\"The mean and std of auc_roc_weighted are: {np.array(model_auc_roc_weighted).mean()}, and {np.array(model_auc_roc_weighted).std()}\")\n",
    "logger.info(f\"The mean and std of f1_macro are: {np.array(model_f1_macro).mean()}, and {np.array(model_f1_macro).std()}\")\n",
    "logger.info(f\"The mean and std of f1_micro are: {np.array(model_f1_micro).mean()}, and {np.array(model_f1_micro).std()}\")\n",
    "logger.info(f\"The mean and std of f1_weighted are: {np.array(model_f1_weighted).mean()}, and {np.array(model_f1_weighted).std()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32caa8d1e81c8fa2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3de9a6f8ab5d416c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
